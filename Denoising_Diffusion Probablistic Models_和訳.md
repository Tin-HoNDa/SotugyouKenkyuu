# ノイズ除去拡散確率モデル

## 目次 <!-- omit in toc -->

- [ノイズ除去拡散確率モデル](#ノイズ除去拡散確率モデル)
  - [概要](#概要)
  - [1 はじめに](#1-はじめに)
  - [2 背景](#2-背景)
  - [3 拡散モデルとノイズ除去オートエンコーダー](#3-拡散モデルとノイズ除去オートエンコーダー)
    - [3.1 順工程と$L\_T$](#31-順工程とl_t)
    - [3.2 逆工程と$L\_{1:T-1}$](#32-逆工程とl_1t-1)
    - [3.3 データのスケーリング、逆工程デコーダーと$L\_0$](#33-データのスケーリング逆工程デコーダーとl_0)
    - [3.4 学習目的の簡素化](#34-学習目的の簡素化)
  - [4 実験](#4-実験)
    - [4.1 サンプルの品質](#41-サンプルの品質)
    - [4.2 逆工程のパラメータ化と学習目的アブレーション](#42-逆工程のパラメータ化と学習目的アブレーション)
    - [4.3 階層符号化](#43-階層符号化)
      - [段階的非可逆圧縮](#段階的非可逆圧縮)
      - [プログレッシブ・ジェネレーション](#プログレッシブジェネレーション)
      - [自己回帰復号への接続](#自己回帰復号への接続)
    - [4.4 補間](#44-補間)
  - [5 関連研究](#5-関連研究)
  - [6 結論](#6-結論)
  - [より大きな影響](#より大きな影響)
  - [謝辞および資金提供の開示](#謝辞および資金提供の開示)
  - [追加情報](#追加情報)
    - [LSUN](#lsun)
    - [段階的圧縮](#段階的圧縮)
  - [A 拡張された導出](#a-拡張された導出)
  - [B 実験内容](#b-実験内容)
  - [C 関連研究についての議論](#c-関連研究についての議論)
  - [D サンプル](#d-サンプル)
    - [追加サンプル](#追加サンプル)
    - [潜在構造と逆過程の確率性](#潜在構造と逆過程の確率性)
    - [粗から細への補間](#粗から細への補間)
  - [参考](#参考)


## 概要

我々は、非平衡熱力学の考察から着想を得た潜在変数モデルの一種である拡散確率モデルを用いた高品質な画像合成結果を発表する。我々の最良の結果は、拡散確率モデルとランジュヴィンダイナミクスを用いたノイズ除去スコアマッチングとの間の新しい接続に従って設計された重み付き変分境界で学習することにより得られる。無条件のCIFAR10データセットにおいて、我々は9.46のInceptionスコアと3.17の最先端FIDスコアを得た。256x256のLSUNでは、ProgressiveGANと同様のサンプル品質が得られた。我々の実装は[こちら](https://github.com/hojonathanho/diffusion。)

## 1 はじめに

最近、あらゆる種類の深層生成モデルが、様々なデータモダリティにおいて高品質なサンプルを示している。生成的敵対ネットワーク（GAN）、自己回帰モデル、フロー、変分オートエンコーダ（VAE）は、印象的な画像や音声サンプルを合成してきた[^14] [^27] [^3] [^58] [^38] [^25] [^10] [^32] [^44] [^57] [^26] [^33] [^45]。また、GANに匹敵する画像を生成する、エネルギーベースのモデリングやスコアマッチングの進歩も目覚ましい[^11] [^55]。

本稿では、拡散確率モデル[^53]の進展を紹介する。拡散確率モデル（ここでは簡潔に「拡散モデル」と呼ぶ）は、変分推論を用いて学習されたパラメータ化されたマルコフ連鎖であり、有限時間後にデータと一致するサンプルを生成する。この連鎖の遷移は拡散過程を逆行するように学習される。拡散過程はマルコフ連鎖であり、信号が破壊されるまで、サンプリングとは逆方向にデータにノイズを徐々に加える。拡散が少量のガウシアンノイズで構成される場合、サンプリングチェーンの遷移も条件付きガウシアンに設定すれば十分であり、特に簡単なニューラルネットワークのパラメータ化が可能になる。

拡散モデルは定義が簡単で訓練も効率的であるが、我々の知る限り、高品質のサンプルを生成できるという実証はなされていない。我々は、拡散モデルが実際に高品質なサンプルを生成できることを示し、時には他のタイプの生成モデルに関する発表結果よりも優れていることを示す（[4章](#4-実験)）。さらに、拡散モデルのあるパラメータ化によって、学習中の複数のノイズレベルにわたるノイズ除去スコアマッチングと、サンプリング中のアニールされたランジュバン動力学との等価性が明らかになることを示す（[3.2節](#32-逆工程とl_1t-1)）[^55] [^61]。我々はこのパラメタリゼーションを用いて最高のサンプル品質結果を得たので（[4.2節](#42-逆工程のパラメータ化と学習目的アブレーション)）、この等価性を我々の主要な貢献の一つと考える。

サンプルの品質にもかかわらず、我々のモデルは他の尤度ベースのモデルと比較して競争力のある対数尤度を持たない（しかし、我々のモデルは、エネルギーベースのモデルやスコアマッチング[^11] [^55]に対してアニールされた重要度サンプリングが生成することが報告されている大きな推定値よりも優れた対数尤度を持つ）。我々は、我々のモデルのロスレスコード長の大部分が、知覚できない画像の細部を記述するために消費されていることを発見した（[4.3節](#43-階層符号化)）。我々はこの現象について、非可逆圧縮の言語でより洗練された分析を行い、拡散モデルのサンプリング手順が、自己回帰モデルで通常可能なことを大幅に一般化したビット順序に沿った自己回帰復号に類似したプログレッシブ復号の一種であることを示す。

## 2 背景

拡散モデル[^53]は、$p_\theta(x_0):=\int p_\theta(x_{0:T})dx_{1:T}$ という形の潜在変数モデルで、$x_1,\cdots,x_T$ はデータ $x_0～p(x_0)$ と同じ次元の潜在である。共同分布 $p_\theta(x_{0:T})$ は逆プロセスと呼ばれ、$p(x_T)=\mathcal{N}(x_T;0,I)$ から始まる学習されたガウス遷移を持つマルコフ連鎖として定義されます。

$$\begin{align*}p_\theta(x_{0:T})&:=p(x_T)\prod^T_{t=1}p_\theta(x_{t-1}|x_t),\\p_\theta(x_{t-1}|x_t)&:=\mathcal{N}(x_{t-1};\mu_\theta(x_t,t),\Sigma_\theta(x_t,t))\end{align*}\tag{1}$$

## 3 拡散モデルとノイズ除去オートエンコーダー

### 3.1 順工程と$L_T$

### 3.2 逆工程と$L_{1:T-1}$

### 3.3 データのスケーリング、逆工程デコーダーと$L_0$

### 3.4 学習目的の簡素化

## 4 実験

### 4.1 サンプルの品質

### 4.2 逆工程のパラメータ化と学習目的アブレーション

### 4.3 階層符号化

#### 段階的非可逆圧縮

#### プログレッシブ・ジェネレーション

#### 自己回帰復号への接続

### 4.4 補間

## 5 関連研究

## 6 結論

## より大きな影響

## 謝辞および資金提供の開示

## 追加情報

### LSUN

### 段階的圧縮

## A 拡張された導出

## B 実験内容

## C 関連研究についての議論

## D サンプル

### 追加サンプル

### 潜在構造と逆過程の確率性

### 粗から細への補間

## 参考
