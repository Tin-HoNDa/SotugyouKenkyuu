# 潜在拡散モデルによる高解像度画像合成

## 目次

- [潜在拡散モデルによる高解像度画像合成](#潜在拡散モデルによる高解像度画像合成)
  - [目次](#目次)
  - [概要](#概要)
  - [1 はじめに](#1-はじめに)
    - [高解像度画像合成の民主化](#高解像度画像合成の民主化)
    - [潜在空間への旅立ち](#潜在空間への旅立ち)
  - [2 関連研究](#2-関連研究)
    - [画像合成のための生成モデル](#画像合成のための生成モデル)
    - [2段階画像合成](#2段階画像合成)
  - [3 手法](#3-手法)
    - [3.1 知覚的画像圧縮](#31-知覚的画像圧縮)
    - [3.2 潜伏拡散モデル](#32-潜伏拡散モデル)
      - [拡散モデル](#拡散モデル)
      - [潜在表現の生成モデリング](#潜在表現の生成モデリング)
    - [3.3 条件付けメカニズム](#33-条件付けメカニズム)
  - [4 実験](#4-実験)
    - [4.1 知覚圧縮のトレードオフについて](#41-知覚圧縮のトレードオフについて)
    - [4.2 潜在拡散を利用した画像生成](#42-潜在拡散を利用した画像生成)
    - [4.3 条件付き潜伏拡散](#43-条件付き潜伏拡散)
      - [4.3.1 LDM用トランスフォーマーエンコーダー](#431-ldm用トランスフォーマーエンコーダー)
      - [4.3.2 256^2^を超えるコンボリューショナルサンプリング](#432-2562を超えるコンボリューショナルサンプリング)
    - [4.4 潜在拡散による超解像](#44-潜在拡散による超解像)
    - [4.5 潜像拡散を利用したインペインティング](#45-潜像拡散を利用したインペインティング)
  - [5 限界と社会的影響](#5-限界と社会的影響)
    - [限界](#限界)
    - [社会的影響](#社会的影響)
  - [6 結論](#6-結論)
  - [A 更新履歴](#a-更新履歴)
  - [B ノイズ除去拡散モデルの詳細情報](#b-ノイズ除去拡散モデルの詳細情報)
  - [C 画像ガイドの仕組み](#c-画像ガイドの仕組み)
  - [D その他の成果](#d-その他の成果)
    - [D.1 高解像度合成のための信号対雑音比の選択](#d1-高解像度合成のための信号対雑音比の選択)
    - [D.2 ファーストステージの全モデル一覧](#d2-ファーストステージの全モデル一覧)
    - [D.3 レイアウトから画像への合成](#d3-レイアウトから画像への合成)
    - [D.4 ImageNet上のクラス条件付き画像合成](#d4-imagenet上のクラス条件付き画像合成)
    - [D.5 サンプルの品質とV100Daysの比較（4.1項の続き）](#d5-サンプルの品質とv100daysの比較41項の続き)
    - [D.6 超解像](#d6-超解像)
      - [D.6.1 LDM-BSR：多様な画像劣化を考慮した汎用SRモデル](#d61-ldm-bsr多様な画像劣化を考慮した汎用srモデル)
  - [E 実装の詳細とハイパーパラメータ](#e-実装の詳細とハイパーパラメータ)
    - [E.1 ハイパーパラメータ](#e1-ハイパーパラメータ)
    - [E.2 実装内容](#e2-実装内容)
      - [E.2.1 条件付きLDMのためのτθの実装](#e21-条件付きldmのためのτθの実装)
      - [E.2.2 インペインティング](#e22-インペインティング)
    - [E.3 評価内容](#e3-評価内容)
      - [E.3.1 無条件およびクラス条件付き画像合成における定量的な結果](#e31-無条件およびクラス条件付き画像合成における定量的な結果)
      - [E.3.2 テキストから画像への合成](#e32-テキストから画像への合成)
      - [E.3.3 レイアウトから画像への合成](#e33-レイアウトから画像への合成)
      - [E.3.4 超解像](#e34-超解像)
      - [E.3.5 効率性分析](#e35-効率性分析)
      - [E.3.6 ユーザースタディ](#e36-ユーザースタディ)
  - [F 計算機要件](#f-計算機要件)
  - [G オートエンコーダーモデルの詳細](#g-オートエンコーダーモデルの詳細)
    - [潜在空間における拡散モデルの学習](#潜在空間における拡散モデルの学習)
  - [H その他の定性的結果](#h-その他の定性的結果)

## 概要

拡散モデル（DM）は、画像形成プロセスをノイズ除去オートエンコーダの逐次適用に分解することで、画像データおよびそれ以外のデータに対して最先端の合成結果を実現する。また、その定式化により、再学習することなく画像生成プロセスを制御するためのガイド機構を実現することができる。しかし、これらのモデルは通常ピクセル空間で直接動作するため、強力なDMの最適化はしばしば数百GPU日を消費し、推論は逐次評価により高価となる。DMの品質と柔軟性を維持しつつ、限られた計算資源でDMの学習を可能にするために、我々は、強力な事前学習済みオートエンコーダの潜在空間にDMを適用する。従来の研究とは異なり、このような表現で拡散モデルを学習することで、初めて複雑さの軽減と細部の保存の間の最適に近い点に到達し、視覚的忠実度を大幅に向上させることができる。また、クロスアテンションレイヤーをモデルアーキテクチャに導入することで、拡散モデルをテキストやバウンディングボックスのような一般的な条件入力に対する強力で柔軟なジェネレーターに変え、畳み込み方式で高解像度合成を可能にする。我々の潜在拡散モデル（LDM）は、ピクセルベースのDMと比較して計算量を大幅に削減しながら、画像インペインティングとクラス条件付き画像合成で新たな最先端スコアを達成し、テキストから画像合成、無条件画像生成、超解像を含む様々なタスクで高い競争力を発揮する。

## 1 はじめに

画像合成は、コンピュータビジョン分野の中で最も目覚しい発展を遂げている分野の一つであるが、同時に最も計算量の多い分野の一つでもある。特に複雑で自然なシーンの高解像度合成は、現在、自己回帰（Autoregressive, AR）変換器の数十億のパラメータを含む尤度ベースモデルのスケールアップが主流となっている[^66] [^67]。一方、GAN [^3] [^27] [^40]の期待できる結果は、その敵対的学習手順が複雑なマルチモーダル分布のモデリングに容易に拡張できないため、ほとんどが比較的に変動性の低いデータに限られていることが明らかにされている。最近では、ノイズ除去オートエンコーダの階層から構築される拡散モデル [^82] が、画像合成 [^30] [^85] 以降 [^7] [^45] [^48] [^57] において印象的な結果を達成し、クラス条件付き画像合成 [^31] や超解像 [^72] の最先端を定義していることが示されている。さらに、他のタイプの生成モデル[^19] [^46] [^69]とは対照的に、無条件DMでさえ、インペインティングやカラー化[^85]、ストロークベースの合成[^53]といったタスクに容易に適用することができる。尤度ベースモデルであるため、GANのようなモード崩壊や学習不安定性がなく、また、パラメータ共有を多用することで、ARモデルのように何十億ものパラメータを必要とせず、自然画像の非常に複雑な分布をモデル化できる [^67] 。

### 高解像度画像合成の民主化

DMは尤度ベースモデルのクラスに属し、そのモードカバー動作により、データの知覚できない詳細をモデル化するために過剰な容量（したがって計算リソース）を費やす傾向がある[^16] [^73]。再重み付け変分目的 [^30] は、最初のノイズ除去ステップをアンダーサンプリングすることでこの問題に対処することを目的としているが、このようなモデルの訓練と評価には、RGB画像の高次元空間における関数評価（および勾配計算）の繰り返しが必要となるため、DMは依然として計算負荷が高い。例えば、最も強力なDMの学習には数百GPU日（例えば[^15]では150～1000V100日）かかることが多く、入力空間のノイズバージョンで繰り返し評価することで推論も高くなるため、5万サンプルを作成するにはA100GPU1台で約5日[^15]かかる。このことは、研究コミュニティや一般ユーザーにとって2つの結果をもたらす。第一に、このようなモデルの学習には、研究分野のごく一部にしか利用できない大規模な計算資源が必要であり、膨大なカーボンフットプリントが残る[^65] [^86]。第二に、すでに訓練されたモデルを評価することは、同じモデルアーキテクチャを多数のステップ（例えば、[^15]では25〜1000ステップ）で連続して実行しなければならないため、時間とメモリにコストがかかる。

この強力なモデルクラスへのアクセス性を高め、同時にその多大な資源消費量を削減するためには、学習とサンプリングの両方の計算量を削減する方法が必要である。したがって、DMの性能を損なわずに計算量を減らすことは、DMの利用しやすさを向上させる鍵となる。

### 潜在空間への旅立ち

私たちのアプローチは、画素空間ですでに訓練された拡散モデルの分析から始まる。図2は、学習済みモデルのレートとディストーションのトレードオフを示したものである。他の尤度ベースモデルと同様に、学習は大きく2つの段階に分けられる。第一段階は知覚的圧縮で、高周波の細部を除去するが、意味的な変化はほとんど学習されない。第二段階では、実際の生成モデルがデータの意味的・概念的な構成要素を学習する（意味的圧縮）。このように、我々はまず、高解像度画像合成のための拡散モデルを訓練するために、知覚的に同等であるが計算上より適した空間を見つけることを目的としている。

一般的な方法 [^11] [^23] [^66] [^67] [^96] に従って、我々は学習を2つの異なるフェーズに分ける。まず、データ空間と知覚的に等価な低次元の表現空間を提供するオートエンコーダを訓練する。重要なことは、先行研究[^23] [^66]とは対照的に、空間次元に関してより優れたスケーリング特性を示す学習済み潜在空間でDMを訓練するため、過度の空間圧縮に頼る必要がないことである。また、複雑さが軽減されたことで、1回のネットワークパスで潜在空間から効率的に画像を生成することができる。この結果、潜在拡散モデル(LDMs)と呼ばれるモデルクラスが誕生した。

このアプローチの顕著な利点は、普遍的な自動符号化ステージを一度だけ訓練する必要があるため、複数のDM訓練や、おそらく全く異なるタスクの探索のために再利用できることである[^81]。これにより、様々な画像から画像、テキストから画像へのタスクに対して、多数の拡散モデルを効率的に探索することができる。後者については、変換器をDMのUNetバックボーン[^71]に接続し、任意のタイプのトークンベースの条件付け機構を可能にするアーキテクチャを設計する（[3.3節](#33-条件付けメカニズム)を参照）。

まとめると、我々の仕事は以下のような貢献をしている。

1. 純粋な変換器ベースのアプローチ[^23] [^66]とは対照的に、本手法は高次元のデータに対してより優雅にスケールするため、
   a. 従来よりも忠実で詳細な再構成を提供する圧縮レベルで動作し（図1参照）、
   b. メガピクセル画像の高解像度合成に効率的に適用することが可能である。
2. 我々は、複数のタスク（無条件画像合成、インペインティング、確率的超解像）およびデータセットにおいて、計算コストを大幅に削減しながら、競争力のある性能を達成した。また、ピクセルベースの拡散アプローチと比較して、推論コストを大幅に削減することができる。
3. エンコーダ／デコーダアーキテクチャとスコアベースの事前学習を同時に行う先行研究[^93]とは対照的に、我々のアプローチでは再構成能力と生成能力の微妙な重み付けを必要としないことを示している。これにより、極めて忠実な再構成が保証され、潜在空間の正則化もほとんど必要ない。
4. 超解像、インペインティング、意味合成などの高密度な条件を持つタスクに対して、本モデルを畳み込み方式で適用することで、約 $1024^2$ pxの大規模で一貫した画像をレンダリングできることを見出した。
5. さらに、クロスアテンションに基づく汎用的な条件付け機構を設計し、マルチモーダルな学習を可能にした。これを用いて、クラス条件付けモデル、テキストから画像への変換モデル、レイアウトから画像への変換モデルを学習する。
6. 最後に、DMの学習以外にも様々なタスクに再利用可能な潜在拡散モデルや自動符号化モデルを[https://github.com/CompVis/latent-diffusion]で事前学習して公開する[^81]。

## 2 関連研究

### 画像合成のための生成モデル

画像の高次元の性質は、生成的なモデリングに明確な課題を与える。Generative Adversarial Networks (GAN) [^27]は、高解像度の画像を効率よくサンプリングすることができ、知覚的な品質も高い[^3] [^42]が、最適化が難しく[^2] [^28] [^54] 、データの分布を完全に把握することに苦労する[^55]。これに対し、尤度ベースの手法は、密度推定を重視するため、最適化がよりうまくいく。変分オートエンコーダ（VAE）[^46]やフローベースモデル[^18] [^19]は、高解像度画像の効率的な合成を可能にするが[^9] [^44] [^92]、サンプル品質はGANと同程度ではない。自己回帰モデル（ARM）[^6] [^10] [^94] [^95]は密度推定で高い性能を発揮するが、計算量の多いアーキテクチャ[^97]と連続したサンプリングプロセスにより、低解像度画像に限定される。画像のピクセルベースの表現には、ほとんど知覚できない高周波の詳細が含まれているため[^16] [^73]、最尤訓練はそのモデル化に不釣り合いな量の容量を費やし、結果として長い訓練時間を要することになる。より高い解像度に対応するため、いくつかの2段階アプローチ [^23] [^67] [^101] [^103] では、ARMを使用して、生のピクセルではなく、圧縮された潜在画像空間をモデル化する。

最近、**拡散確率モデル** (Diffusion Probabilistic Models) [^82]は、密度推定[^45]やサンプル品質[^15]で最先端の結果を達成している。これらのモデルの生成能力は、その基礎となるニューラル・バックボーンが U-Net として実装されている場合に、画像類似データの帰納的バイアスに自然に適合することに起因する[^15] [^30] [^71] [^85]。最良の合成品質は、通常、再重量化された目的語[^30]を学習に使用したときに達成される。この場合、DMは非可逆圧縮機に相当し、画質と圧縮能力を交換することができる。しかし、これらのモデルをピクセル空間で評価・最適化すると、推論速度が遅く、学習コストが非常に高くなるという欠点がある。前者は高度なサンプリング戦略[^47] [^75] [^84]や階層的アプローチ[^31] [^93]によって部分的に対処できるが、高解像度画像データでの学習には常に高価な勾配の計算が必要である。我々は、低次元の圧縮された潜在空間で動作するLDMを提案することで、この2つの欠点に対処する。これにより、計算量が少なくなり、合成の質をほとんど落とさずに推論を高速化することができる（図1参照）。

### 2段階画像合成

個々の生成アプローチの欠点を軽減するために、多くの研究[^11] [^23] [^67] [^70] [^101] [^103]が、2段階のアプローチによって、異なる手法の長所を組み合わせて、より効率的で性能の高いモデルを作ることに取り組んでいる。VQ-VAE[^67] [^101]は、自己回帰モデルを用いて、離散化された潜在空間に対する表現的な事前分布を学習する[^66]。このアプローチをテキスト-画像生成に拡張し、離散化した画像とテキスト表現に対する共同分布を学習する。より一般的には、[^70]は、条件付き反転ネットワークを用いて、多様なドメインの潜在空間間の汎用的な転送を提供している。VQ-VAEとは異なり、VQGAN [^23] [^103]は、自己回帰変換器をより大きな画像に拡大するために、敵対的かつ知覚的な目的を持つ第1段階を採用している。しかし、実現可能なARM学習に必要な高い圧縮率は、何十億もの学習可能なパラメータを導入するため[^23] [^66]、このようなアプローチの全体的な性能を制限し、より少ない圧縮は高い計算コストと引き換えになる[^23] [^66]。我々の提案するLDMは、畳み込みバックボーンにより、高次元の潜在空間に対してより優しくスケールするため、このようなトレードオフを防ぐことができる。したがって、我々は、高忠実度の再構成を保証しながら、生成拡散モデルに知覚的圧縮を任せすぎず、強力な第1段階の学習を最適に仲介する圧縮レベルを自由に選択することができる（図1参照）。

スコアベースの事前学習とともに符号化・復号化モデルを学習するアプローチ[^93]や個別学習するアプローチ[^80]が存在するが、前者は再構成能力と生成能力の間の難しい秤量が必要であり[^11]、我々のアプローチ（第4項）に劣り、後者は人間の顔などの高度な構造の画像に焦点を当てている。

## 3 手法

高解像度画像合成のために拡散モデルを学習する際の計算量を減らすために、拡散モデルは対応する損失項をアンダーサンプリングすることで知覚的に無関係な細部を無視することができるが[^30]、ピクセル空間での高価な関数評価を必要とし、計算時間やエネルギー資源に大きな需要があることを確認した。

我々は、この欠点を回避するために、圧縮学習と生成学習を明示的に分離することを提案する（図2参照）。これを実現するために、画像空間と知覚的に等価な空間を学習する自動符号化モデルを利用し、計算量を大幅に削減する。

このようなアプローチにはいくつかの利点がある。

1. 高次元の画像空間から離れることで、低次元の空間でサンプリングが行われるため、計算効率が非常に高いDMを得ることができる。
2. U-Netアーキテクチャ[^71]に由来するDMの帰納的バイアスを利用することで、空間構造を持つデータに対して特に有効であるため、従来のアプローチ [^23] [^66]で必要とされた、積極的で品質を下げる圧縮レベルの必要性が緩和される。
3. 最後に、潜在空間を複数の生成モデルの学習に利用でき、単一画像CLIPガイド付き合成[^25]などの他の下流アプリケーションに利用できる汎用的な圧縮モデルを得ることができる。

### 3.1 知覚的画像圧縮

我々の知覚圧縮モデルは、以前の研究 [^23] に基づいており、知覚損失 [^106] とパッチベース [^33] 敵対的目的 [^20] [^23] [^103] の組み合わせによって訓練されたオートエンコーダで構成されている。これにより、局所的なリアリズムを強制することで再構成が画像多様体に限定されることを保証し、$L_2$ や $L_1$ 目的などの画素空間損失のみに依存することで生じるぼやけを回避する。

より正確には、RGB空間の画像 $x\in \mathbb{R}^{H\times W\times3}$ が与えられたとき、エンコーダ $\mathcal{E}$ は $x$ を潜在表現 $z=\mathcal{E}(x)$ に符号化し、デコーダ $\mathcal{D}$ は潜在表現から画像を再構成し、$\tilde x=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))\space(z\in \mathbb{R}^{H\times W\times3})$ を与える。重要なのは、エンコーダが画像を係数 $f=H/h=W/w$ でダウンサンプリングすることであり、我々は異なるダウンサンプリング係数 $f=2^m\space(m\in\mathbb{N})$ で調査する。

任意に高変量な潜在空間を避けるために、2つの異なる正則化の実験を行った。最初のバリエーションであるKL-reg.は、VAE [^46] [^69]と同様に、学習した潜在能力に対して標準正規形に対するわずかなKL-penaltyを課すが、VQ-reg.はデコーダ内のベクトル量子化層 [^96] を用いる。このモデルはVQGAN [^23]と解釈できるが、量子化層がデコーダに吸収されている。我々の後続のDMは、学習した潜在空間 $z=\mathcal{E}(x)$ の2次元構造を扱うように設計されているため、比較的穏やかな圧縮率を使用し、非常に優れた再構成を達成することができる。これは、学習した空間 $z$ の分布を自己回帰的にモデル化するために、任意の1次元の順序に依存し、それによって $z$ の固有の構造の多くを無視した先行研究 [^23] [^66] とは対照的である。したがって、我々の圧縮モデルは $x$ の詳細をよりよく保存する（表8参照）。目的および学習の詳細については付録を参照。

### 3.2 潜伏拡散モデル

#### 拡散モデル

**拡散モデル**[^82]は、正規分布する変数を徐々にノイズ除去することによってデータ分布 $p(x)$ を学習するように設計された確率的モデルであり、これは長さ $T$ の固定マルコフ連鎖の逆過程を学ぶことに相当する。画像合成については、最も成功したモデル[^15] [^30] [^72]は、ノイズ除去スコアマッチ[^85]を反映する $p(x)$ の変分下限の再可重化に依拠している。これらのモデルは、入力 $x_t$（$x_t$ は入力 $x$ のノイズバージョン）のノイズ除去されたバリエーションを予測するように訓練されたノイズ除去オートエンコーダ $\epsilon_\theta\left(x_t,t\right);t=1,\cdots,T$ の等しい重み付けシーケンスとして解釈することができる。対応する目的は、$t$ を $\{1,\cdots,T\}$ から均一にサンプリングして（[項B](#b-ノイズ除去拡散モデルの詳細情報)）

$$L_{DM}=\mathbb{E}_{x,\epsilon～\mathcal{N}(0,1),t}\left[\left|\left|\epsilon-\epsilon_\theta\left(x_t,t\right)\right|\right|^2_2\right]\tag{1}$$

に単純化することができる。

#### 潜在表現の生成モデリング

$\mathcal{E}$ と $\mathcal{D}$ からなる知覚圧縮モデルの学習により、高周波数で知覚できない細部が抽象化された効率的で低次元の潜在空間を利用できるようになった。この空間は、高次元の画素空間と比較して、尤度ベースの生成モデルに適しており、（i）データの重要な意味的ビットに焦点を当て、（ii）低次元で計算効率の高い空間で訓練することができる。

高度に圧縮された離散的な潜在空間における自己回帰的な注意ベースの変換モデル[^23] [^66] [^103] に依存した以前の研究とは異なり、我々のモデルが提供する画像特有の帰納的バイアスを利用することができる。これには、主に2次元畳み込み層から基礎となるU-Netを構築する能力と、再重み付け境界を使用して知覚的に最も関連性の高いビットに目的をさらに集中させる能力が含まれ、現在では

$$L_{LDM}:=\mathbb{E}_{\mathcal{E}\left(x\right),\epsilon～\mathcal{N}\left(0,1\right),t}\left[\left|\left|\epsilon-\epsilon_\theta\left(z_t,t\right)\right|\right|^2_2\right]\tag{2}$$

のように読み取れる。このモデルのニューラルバックボーン $\epsilon_\theta(◦,t)$ は、時間条件付きU-Net[^71]として実現されている。  前進過程が固定であるため、$z_t$ は訓練中に $\mathcal{E}$ から効率的に得ることができ、$p(z)$ からのサンプルは $\mathcal{D}$ を1回通過するだけで画像空間に復号化されることが可能である。

### 3.3 条件付けメカニズム

他のタイプの生成モデル[^56] [^83]と同様に、拡散モデルは原理的に $p(z|y)$ の形の条件付き分布をモデル化することが可能である。これは条件付きノイズ除去オートエンコーダ $\epsilon_\theta(z_t,t,y)$ で実装でき、テキスト[^68]、意味マップ[^33] [^61]、その他の画像間翻訳タスクなどの入力 $y$ を通して合成プロセスを制御する道を開く[^34]。

しかし、画像合成の文脈では、クラスラベル[^15]や入力画像の不鮮明なバリエーション[^72]以外の他のタイプの条件とDMの生成力を組み合わせることは、今のところ未開拓の研究分野である。

我々は、DMをより柔軟な条件付き画像生成器とするために、その基礎となるU-Netバックボーンを、様々な入力モダリティの注意に基づくモデルの学習に有効なcross-attentionメカニズム[^97]で補強する[^35] [^36]。様々なモダリティ（言語プロンプトなど）からの $y$ を前処理するために、$y$ を中間表現 $\tau_\theta(y)\in \mathbb{R}^{M\times d_\tau}$ に投影するドメイン固有エンコーダ $\tau_\theta$ を導入し、これをAttention$(Q,K,V)=$ softmax$\left(\frac{QK^T}{\sqrt{d}}\right)\cdot V$ を実装したクロスアテンション層を介してU-Netの中間層へマッピングし、

$$Q=W_Q^{(i)}\cdot\varphi_i\left(z_t\right),K=W^{(i)}_K\cdot\tau_\theta\left(y\right),V=W_V^{(i)}\cdot\tau_\theta(y)$$

とする。

ここで、$\varphi_i(z_t)\in\mathbb{R}^{N×d^i_\epsilon}$ は $\epsilon_\theta$ を実装したU-Netの（平坦化された）中間表現を示し、$W^{(i)}_V\in\mathbb{R}^{d×d^i_\epsilon},W^{(i)}_Q\in\mathbb{R}^{d×d_\tau},W^{(i)}_K\in\mathbb{R}^{d×d_\tau}$ は学習型射影行列[^36] [^97]を表す。視覚的な描写については、図3を参照。

画像とコンディショニングのペアに基づき、

$$L_{LDM}:=\mathbb{E}_{\mathcal{E}\left(x\right),y,\epsilon～\mathcal{N}\left(0,1\right),t}\left[\left|\left|\epsilon-\epsilon_\theta\left(z_t, t, \tau_\theta\left(y\right)\right)\right|\right|^2_2\right]\tag{3}$$

により条件付きLDMを学習し、$\tau_\theta$ と $\epsilon_\theta$ は式3により共同最適化される。この条件付けのメカニズムは柔軟で、$\tau_\theta$ はドメイン固有のエキスパート、例えば $y$ がテキストプロンプトの場合は（マスクされていない）変換器[^97]でパラメータ化できる（[4.3.1項](#431-ldm用トランスフォーマーエンコーダー)参照）。

## 4 実験

LDMは、様々な画像モダリティの拡散に基づく画像合成を柔軟かつ計算しやすくする手段を提供し、我々はそれを以下のように実証的に示す。まず、学習と推論の両方において、ピクセルベースの拡散モデルと比較して、我々のモデルの利点を分析する。興味深いことに、VQ正則化された潜在空間で学習したLDMは、VQ正則化された初段モデルの再構成能力が連続モデルの再構成能力にわずかに及ばないにもかかわらず、より良いサンプル品質を達成する場合があることがわかった（表8）。 LDM学習における第一段階の正則化スキームの効果と、解像度$256^2$以上への一般化能力の視覚的比較は、付録[D.1](#d1-高解像度合成のための信号対雑音比の選択)に記載されている。[E.2](#e2-実装内容)には、このセクションで紹介したすべての結果について、アーキテクチャ、実装、学習、評価の詳細を列挙している。

### 4.1 知覚圧縮のトレードオフについて

このセクションでは、異なるダウンサンプリング係数 $f\in{1,2,4,8,16,32}$（LDM-fと略記、LDM-1はピクセルベースのDMに対応）を持つ我々のLDMの挙動を解析する。比較可能なテストフィールドを得るために、本節のすべての実験において、計算資源を単一のNVIDIA A100に固定し、すべてのモデルを同じステップ数、同じパラメータ数で訓練する。

表8は、本節で比較したLDMに用いた初段モデルのハイパーパラメータと再構成性能である。図6は、ImageNet [^12]データセットにおけるクラス条件モデルの2Mステップの学習進度の関数としてのサンプル品質である。LDM-{1,2}のダウンサンプリング係数が小さいと学習の進捗が遅くなり、$f$ の値が大きすぎると学習ステップ数が少なくなり、忠実度が低下することがわかる。これは、i)知覚的圧縮の大部分を拡散モデルに委ね、ii)第一段階の圧縮が強すぎて情報損失が生じ、達成可能な品質が制限されるためである、と上記分析（図1、2）を再検討している。LDM-{4-16}は、効率と知覚に忠実な結果の間で良いバランスを取っており、これは、2M学習ステップ後のピクセルベース拡散（LDM-1）とLDM-8の間のFID [^29] ギャップ38という大きな差となって現れている。

図7では、CelebAHQ [^39]とImageNetで学習したモデルを、DDIMサンプラー[^84]を用いて異なるノイズ除去ステップ数のサンプリング速度で比較し、FIDスコア[^29]に対してプロットしている。LDM-{4-8}は、知覚的・概念的な圧縮率が不適当なモデルを凌駕している。特にピクセルベースのLDM-1と比較すると、サンプルのスループットを大幅に向上させながら、はるかに低いFIDスコアを達成している。ImageNetのような複雑なデータセットでは、品質の低下を避けるために圧縮率を下げる必要がある。以上のことから、LDM-4と-8は、高品質な合成結果を得るための最適な条件を備えていると言える。

### 4.2 潜在拡散を利用した画像生成

CelebAHQ [^39], FFHQ [^41], LSUN-Churches, -Bedrooms [^102]の $256^2$ 枚の画像の無条件モデルを学習し、ii) FID [^29] と ii) Precision-and-Recall [^50] を用いて i) サンプル品質と ii) データ多様体のカバー率を評価した。表1がその結果である。1に結果をまとめる。CelebA-HQにおいて、我々は5.11という最新のFIDを報告し、GANと同様に以前の尤度ベースモデルよりも優れている。また、潜在的な拡散モデルを第1ステージと共同で学習させるLSGM [^93]をも凌駕する。これに対して、我々は、拡散モデルを固定空間で学習させ、再構成の品質と潜在空間上の事前学習を比較検討する難しさを回避している（図1-2参照）。

LDMは、LSUN-Bedroomsデータセット以外のすべてのデータセットにおいて、ADM [^15]の半分のパラメータを利用し、4倍少ない訓練リソースを必要とするにもかかわらず、先行する拡散ベースのアプローチを上回った（付録[E.3.5](#e35-効率性分析)を参照）。さらに、LDMはGANベースの手法よりもPrecisionとRecallで一貫して向上しており、モードカバー尤度ベースの学習目的が敵対的アプローチよりも優れていることが確認された。図4では、各データセットの定性的な結果も示している。

### 4.3 条件付き潜伏拡散

#### 4.3.1 LDM用トランスフォーマーエンコーダー

LDMにcross-attentionに基づく条件付けを導入することで、これまで拡散モデルで未開拓だった様々な条件付けの様式に対応できる。テキストから画像への画像モデリングのために、LAION-400M[^78]で言語プロンプトを条件とする1.45Bパラメータの $KL$ 正則化LDMを訓練する。BERT-tokenizer[^14]を採用し、$\tau_\theta$ を変換器[^97]として実装して、（マルチヘッド）クロスアテンションを介してUNetにマッピングされる潜在コードを推測する（[3.3節](#33-条件付けメカニズム)）。このように、言語表現を学習するためのドメイン固有の専門家と視覚的合成を組み合わせることで、図8と図5のように、複雑でユーザー定義のテキストプロンプトによく一般化する、強力なモデルが得られる。定量的な分析については、先行研究に従い、MS-COCO[^51]の検証セットでテキストから画像への生成を評価し、我々のモデルがAR[^17] [^66]やGANベース[^109]の強力な手法に勝ることを示した（表2）。分類器を使わない拡散ガイダンス[^32]を適用することで、LDM-KL-8-Gは、パラメータ数を大幅に削減しながら、テキストから画像への合成のための最近の最先端のAR[^26]や拡散モデル[^59]と同等であるように、サンプル品質を大幅に向上させていることに注目する。クロスアテンションベースの条件付けメカニズムの柔軟性をさらに分析するために、OpenImages[^49]で意味的レイアウトに基づく画像を合成するモデルを訓練し、COCO[^4]で微調整も行った（図8）。定量的な評価と実装の詳細については、[項D.3](#d3-レイアウトから画像への合成)を参照のこと。

最後に、先行研究[^3] [^15] [^21] [^23]に従い、[4.1節](#41-知覚圧縮のトレードオフについて)の $f\in{4,8}$ で最も性能の良いクラス条件付きImageNetモデルを表3で評価した。ここでは、計算量とパラメータ数を大幅に削減しながら、最先端の拡散モデルADM [^15]を上回った（表18参照）。

#### 4.3.2 256^2^を超えるコンボリューショナルサンプリング

空間的に整列した条件付け情報を $\theta$ の入力に連結することで、LDMは効率的な汎用画像間変換モデルとして機能する。これを用いて、意味合成、超解像（[4.4節](#44-潜在拡散による超解像)）、インペインティング（[4.5節](#45-潜像拡散を利用したインペインティング)）のモデルを学習する。意味合成では、意味マップ[^23] [^61]と対になった風景の画像を用い、意味マップのダウンサンプル版と $f=4$ モデルの潜像表現を連結する（VQ-reg.、表8）。入力解像度は $256^2$（$384^2$ からのクロップ）であるが、我々のモデルはより大きな解像度に一般化し、畳み込み方式で評価するとメガピクセル領域までの画像を生成できることがわかった（図9）。この挙動を利用して、[4.4節](#44-潜在拡散による超解像)の超解像モデルと[4.5節](#45-潜像拡散を利用したインペインティング)のインペインティングモデルを適用し、$512^2$ ～ $1024^2$ の大きな画像を生成することもできる。このアプリケーションでは、（潜在空間のスケールによって引き起こされる）$S/N$ 比が結果に大きく影響する。[項D.1](#d1-高解像度合成のための信号対雑音比の選択)では、（i）$f＝4$ モデル（KL-reg.、表8）によって提供される潜在空間と、（ii）成分ごとの標準偏差でスケーリングされた再スケーリング版でLDMを学習する場合に、これを説明した。

後者は、分類器不要のガイダンス[^32]と組み合わせることで、図13のようにテキスト条件付きLDM-KL-8-Gに対して $256^2$ 枚以上の画像を直接合成することも可能である。

### 4.4 潜在拡散による超解像

LDMは、連結によって低解像度画像に直接条件付けすることで、超解像のための学習を効率的に行うことができる（[3.3節](#33-条件付けメカニズム)参照）。最初の実験では、SR3[^72]に従い、画像劣化を4×ダウンサンプリングによるバイキュービック補間に固定し、SR3のデータ処理パイプラインに従ってImageNetで学習する。OpenImagesで事前に学習した $f=4$ 自動符号化モデル（VQ-reg., cf. 表8）を使用し、低解像度条件付け $y$ とUNetへの入力を連結する、つまり $\tau_\theta$ を同一視する。定性的・定量的な結果（図10、表5）は、競争力のある性能を示し、LDM-SRはFIDでSR3を上回り、SR3はISで優れている。しかし、これらの指標は人間の知覚[^106]とあまり一致せず、不完全に位置合わせされた高周波の詳細よりもぼやけた状態を好む[^72]。さらに、ピクセルベースラインとLDM-SRを比較するユーザスタディを実施した。SR3[^72]に従い、2つの高解像度画像の間に低解像度画像を表示し、被験者に好みを聞いている。その結果、表4では、LDM-SRの性能が高いことが確認できる。PSNRとSSIMは、ポストホックガイド機構[^15]を使用することで押し上げることができ、我々はこの画像ベースのガイドを知覚的損失によって実装した（[項D.6](#d6-超解像)）。バイキュービック劣化処理は、この前処理に従わない画像にはうまく一般化できないため、より多様な劣化を用いることで汎用モデルであるLDM-BSRも訓練する。その結果を[D.6節](#d6-超解像)に示す。

### 4.5 潜像拡散を利用したインペインティング

インペインティングとは、画像の一部が破損しているため、あるいは画像内の既存の望ましくないコンテンツを置き換えるために、画像のマスクされた領域を新しいコンテンツで埋めるタスクである。我々は、条件付き画像生成のための我々の一般的なアプローチが、このタスクのためのより専門的で最先端のアプローチと比較してどうであるかを評価する。我々の評価は、高速フーリエ変換[^8]に依存する特殊なアーキテクチャを導入した最近のインペインティングモデルであるLaMa[^88]のプロトコルに従っている。Places[^108]の正確な訓練と評価プロトコルは、E.2.2節で説明されている。

まず、第1段階の異なる設計選択の効果を分析する。特に、LDM-1（すなわちピクセルベースの条件付きDM）のインペイント効率を、$KL$ および $VQ$ 正則化の両方についてLDM-4と比較し、さらに、第1ステージに注意を払わないVQLDM-4も比較する（表8）。なお、後者は高い解像度でのデコード用にGPUメモリを削減する。比較のため、すべてのモデルでパラメータ数を固定している。表6は、解像度 $256^2$ と $512^2$ での訓練とサンプリングのスループット、1エポックあたりの総訓練時間（時間）、6エポック後の検証分割でのFIDスコアの報告である。全体として、ピクセルベースと潜在ベースの拡散モデルの間で少なくとも2.7倍のスピードアップが見られ、同時にFIDスコアは少なくとも1.6倍向上していることが確認された。

表7で他のインペインティングアプローチと比較すると、注意を喚起する我々のモデルは、[^88]のものよりFIDで測定される全体的な画像品質を改善することがわかる。マスクなし画像と我々のサンプルの間のLPIPSは、[^88]のものよりわずかに高い。これは、[^88]が単一の結果しか出さないため、我々のLDMが出す多様な結果と比較して、平均的な画像をより多く回復する傾向があるためと考えられる（図21）。さらに、ユーザー調査（表4）でも、被験者は[^88]の結果よりも我々の結果を支持している。

これらの初期結果に基づいて、我々はまた、より大きな拡散モデル（表7のビッグ）を、注意を伴わない $VQ$ 正則化第1段階の潜在空間で訓練した。この拡散モデルのUNetは、[^15]に従い、特徴階層の3つのレベルに注目層を用い、アップ・ダウンサンプリングにBigGAN[^3]の残差ブロックを用い、215Mの代わりに387Mのパラメータを持っている。学習後、解像度 $256^2$ と $512^2$ で生成されたサンプルの品質に矛盾があることに気づきましたが、これは注意モジュールが追加されたことに起因していると推測される。しかし、解像度 $512^2$ で半エポック分の微調整を行うことで、モデルが新しい特徴統計量に適応し、画像インペインティングで新しい最新鋭のFIDを設定した（表7、図11のbig、w/o attn、w/ ft）。

## 5 限界と社会的影響

### 限界

LDMはピクセルベースのアプローチと比較して計算量を大幅に削減することができるが、その逐次サンプリングプロセスはGANと比較してまだ遅い。さらに、高精度が要求される場合には、LDMの使用には疑問が残る。$f=4$ の自動符号化モデル（図1参照）では、画質の低下は非常に小さいが、ピクセル空間での細かい精度が要求されるタスクでは、その再構成能力がボトルネックとなり得る。この点については、超解像モデル（[4.4節](#44-潜在拡散による超解像)）がすでにある程度制限されていると想定される。

### 社会的影響

画像のようなメディアの生成モデルは、諸刃の剣である。特に、学習と推論のコストを削減する我々のアプローチは、この技術へのアクセスを容易にし、その探求を民主化する可能性を持っている。その一方で、操作されたデータを作成し広めることや、誤った情報やスパムを広めることが容易になることも意味している。特に、画像の意図的な操作（「ディープフェイク」）は、この文脈でよく見られる問題であり、特に女性はその影響を不当に受けている[^13] [^24]。

生成モデルは学習データを公開することも可能であり[^5] [^90]、そのデータが機密情報や個人情報を含み、明示的な同意なしに収集された場合は大きな懸念材料となる。しかし、これが画像のDMにもどの程度適用されるかは、まだ十分に理解されていない。

最後に、深層学習モジュールは、データに既に存在するバイアスを再現したり悪化させたりする傾向がある[^22] [^38] [^91]。拡散モデルは、GANベースのアプローチなどよりもデータ分布の優れたカバレッジを達成するが、敵対的な訓練と尤度ベースの目的を組み合わせた我々の2段階のアプローチが、どの程度データを誤って表現するのかは、依然として重要な研究課題となっている。

深層生成モデルの倫理的考察についてのより一般的で詳細な議論については、例えば[^13]を参照されたい。

## 6 結論

我々は、ノイズ除去拡散モデルの品質を低下させることなく、学習効率とサンプリング効率の両方を大幅に改善する簡単で効率的な方法である潜在的拡散モデルを提示した。これと我々のクロスアテンションコンディショニングメカニズムに基づき、我々の実験は、タスク固有のアーキテクチャを持たない広範囲の条件付き画像合成タスクにおいて、最先端の手法と比較して好ましい結果を示すことができた。

***

## A 更新履歴

ここでは、本論文の本バージョン([https://arxiv.org/abs/2112.10752v2](https://arxiv.org/abs/2112.10752v2))と前バージョン([https://arxiv.org/abs/2112.10752v1](https://arxiv.org/abs/2112.10752v1))との間の変更点を列挙する。

- 我々は、[4.3節](#43-条件付き潜伏拡散)のテキストから画像への合成に関する結果を更新した。また、arXivで我々の研究と同時期([^59] [^109])または後に発表された、このタスクに関する非常に新しい競合手法との比較も行っている[^26]。
- ImageNetにおけるクラス条件付き合成の結果を[4.1節](#41-知覚圧縮のトレードオフについて)、表3で更新した。3（[D.4節](#d4-imagenet上のクラス条件付き画像合成)）を更新し、バッチサイズを大きくしてモデルを再トレーニングした。図26と図27の対応する定性的な結果も更新した。更新されたtext-to-imageとクラス条件付きモデルの両方が、視覚的忠実度を高めるための方策として、分類器なしのガイダンス[^32]を用いるようになった。
- Sahariaら[^72]が提案したスキームに従い、ユーザー調査を実施し、インペインティング（[4.5項](#45-潜像拡散を利用したインペインティング)）および超解像モデル（[第4.4項](#44-潜在拡散による超解像)）の追加評価を行った。
- 本紙に図5を追加、図18を付録に移動、図13を付録に追加。

## B ノイズ除去拡散モデルの詳細情報

拡散モデルは、データサンプル $x_0$ から出発して、$s<t$ のマルコフ構造を持つ順拡散過程 $q$ を

$$q\left(x_t|x_0\right)=\mathcal{N}\left(x_t|\alpha_tx_0,\sigma^2_tI\hspace{-2.5pt}I\tag{4}\right)$$

として定義するシーケンス $(\alpha_t)^T_{t=1}$ 、$(\sigma_t)^T_{t=1}$ からなる信号対雑音比 $SNR(t)=\frac{\alpha^2_t}{\sigma^2_t}$ で指定することができる。

$$\begin{align*} \\
  q\left(x_t|x_s\right)=&\mathcal{N}\left(x_t|\alpha_{t|s}x_s,\sigma^2_{t|s}I\hspace{-2.5pt}I\right)\tag{5}\\
  \alpha_{t|s}=&\frac{\alpha_t}{\alpha_s}\tag{6}\\
  \sigma^2_{t|s}=&\sigma^2_t-\alpha^2_{t|s}\sigma^2_s\tag{7}\\
\end{align*}$$

ノイズ拡散モデルは、このプロセスを時間的に逆行する同様のマルコフ構造で回帰する生成モデル $p(x_0)$ すなわち、

$$p\left(x_0\right)=\int_zp\left(x_\Tau\right)\prod^\Tau_{t=1}p\left(x_{t-1}|x_t\right)\tag{8}$$

のように規定される。そして、このモデルに関連する変分下限（ELBO）は、離散的な時間ステップにわたって

$$\begin{align*}&-\log p\left(x_0\right)\\&\leq\mathbb{KL}\left(q\left(x_\Tau|x_0\right)|p\left(x_\Tau\right)\right)+\sum^\Tau_{t=1}\mathbb{E}_{q\left(x_t|x_0\right)}\mathbb{KL}\left(q\left(x_{t-1}|x_t,x_0\right)|p\left(x_{t-1}|x_t\right)\right)\end{align*}\tag{9}$$

のように分解される。事前分布 $p(x_T)$ は通常、標準正規分布として選択され、ELBOの第1項は最終的な信号対雑音比 $SNR(T)$ にのみ依存する。残りの項を最小化するために、$p(x_{t-1}|x_t)$ をパラメータ化する一般的な選択は、真の事後 $q(x_{t-1}|x_t, x_0)$ で指定するが、未知の $x_0$ を現在のステップ $x_t$ に基づく推定 $x_θ(x_t,t)$ で置換することである。これにより、[^45]

$$\begin{align*}\\
  p\left(x_{t-1}|x_t\right)\coloneqq&q\left(x_{t-1}|x_t,x_\theta\left(x_t,t\right)\right)\tag{10}\\
  =&\mathcal{N}\left(x_{t-1}|\mu_\theta\left(x_t,t\right),\sigma^2_{t|t-1}\frac{\sigma^2_{t-1}}{\sigma^2_t}I\hspace{-2.5pt}I\right)\tag{11}\\
\end{align*}$$

が得られ、平均は

$$\mu_\theta\left(x_t,t\right)=\frac{\alpha_{t|t-1}\sigma^2_{t-1}}{\sigma^2_t}x_t+\frac{\alpha_{t-1}\sigma^2_{t|t-1}}{\sigma^2_t}x_\theta\left(x_t,t\right)\tag{12}$$

と表すことができる。この場合、ELBOの和は

$$\begin{align*}&\sum^T_{t=1}\mathbb{E}_{q\left(x_t|x_0\right)}\mathbb{KL}\left(q\left(x_{t-1}|x_t,x_0\right)|p\left(x_{t-1}\right)\right)\\=&\sum^T_{t=1}\mathbb{E}_{\mathcal{N}\left(\epsilon|0,I\hspace{-2.5px}I\right)}\frac{1}{2}\left(SNR\left(t-1\right)-SNR\left(t\right)\right)\left|\left|x_0-x_\theta\left(\alpha_tx_0+\sigma_t\epsilon,t\right)\right|\right|^2\end{align*}\tag{13}$$

に単純化される。[^30]に倣って、再構成項をノイズ除去目的

$$\left|\left|x_0-x_\theta\left(\alpha_tx_0+\sigma_t\epsilon,t\right)\right|\right|^2=\frac{\sigma^2_t}{\alpha^2_t}\left|\left|\epsilon-\epsilon_\theta(\alpha_tx_0+\sigma_t\epsilon,t)\right|\right|^2\tag{15}$$

として表現する再パラメータ化

$$\epsilon_\theta\left(x_t,t\right)=\left(x_t-\alpha_tx_\theta\left(x_t,t\right)\right)/\sigma_t\tag{14}$$

と、各項を同じ重みにする再重み付けを行い、式（1）とする。

## C 画像ガイドの仕組み

拡散モデルの興味深い特徴は、無条件モデルがテスト時に条件付けできることである[^15] [^82] [^85]。特に[^15]では、拡散過程の各 $x_t$ で学習した分類器 $\log p_\Phi(y|x_t)$ を用いて、ImageNetデータセットで学習した無条件モデルと条件付きモデルの両方をガイドするアルゴリズムを提示した。我々はこの定式化に直接基づいて、ポストホックイメージガイディングを導入する。

固定分散を持つ $\epsilon$ -パラメータ化されたモデルの場合、[^15]で紹介されたガイドアルゴリズムは次のようになる。

$$\hat{\epsilon}\larr\epsilon_\theta\left(z,t\right)+\sqrt{1-\alpha^2_t}\nabla_{z_t}\log p_\Phi\left(y|z_t\right)\tag{16}$$

これは、「スコア」$\theta$ を条件付き分布 $\log p_\Phi(y|z_t)$ で補正する更新と解釈することができる。

これまでのところ、このシナリオは単一クラスの分類モデルにしか適用されていない。我々は、ガイド分布 $p_\Phi(y|T(\mathcal{D}(z_0(z_t))))$ を、ターゲット画像 $y$ を与えられた汎用の画像間翻訳タスクとして再解釈する。ここで $T$ は、手元の画像間翻訳タスクに採用される任意の微分可能な変換、例えば同一性、ダウンサンプリング操作または同様のものとすることができる。

例えば、分散を固定したガウスガイド

$$\log p_\Phi\left(y|z_t\right)=-\frac{1}{2}\left|\left|y-T\left(\mathcal{D}\left(z_0\left(z_t\right)\right)\right)\right|\right|^2_2\tag{17}$$

が $L_2$ 回帰の目的語になると仮定することができる。

図14は、この定式化が、$256^2$ 画像で学習した無条件モデルのアップサンプリングメカニズムとして機能することを示す。ここで、サイズ $256^2$ の絶対サンプルは、$512^2$ 画像の畳み込み合成をガイドし、$T$ は $2\times$ バイキュービックダウンサンプリングとなる。この動機に従って、我々は知覚的類似性ガイドの実験も行い、$L_2$ 目標をLPIPS [106]メトリックに置き換えた（[4.4節](#44-潜在拡散による超解像)参照）。

## D その他の成果

### D.1 高解像度合成のための信号対雑音比の選択

[4.3.2節](#432-2562を超えるコンボリューショナルサンプリング)で述べたように、潜在空間の分散によって引き起こされる信号対雑音比（すなわち、$Var(z)/σ^2_t$）は、畳み込みサンプリングの結果に大きく影響する。例えば、KL正則化モデルの潜在空間でLDMを直接学習させた場合（表8参照）、この比率は非常に高く、逆解像処理の初期段階でモデルが多くの意味的詳細を割り当ててしまうような状態である。一方、[G節](#g-オートエンコーダーモデルの詳細)で説明したように、潜在空間を潜在の成分別標準偏差で再スケーリングすると、SNRは低下する。図15に、意味画像合成のための畳み込みサンプリングへの影響を示す。なお、VQ正則化された空間は分散が1に近いので、再スケーリングする必要はない。

### D.2 ファーストステージの全モデル一覧

OpenImagesデータセットで学習させた様々なオートエンコーディングモデルの完全なリストを表8に示す。

### D.3 レイアウトから画像への合成

ここでは、[4.3.1節](#431-ldm用トランスフォーマーエンコーダー)のレイアウト-画像モデルの定量的評価と追加サンプルを提供する。COCO [^4]とOpenImages [^49]データセットでモデルを訓練し、その後、COCOで追加的に微調整を行った。表9はその結果である。COCOモデルは、その訓練と評価プロトコル[^89]に従うと、レイアウトから画像への合成における最近の最先端モデルの性能に到達する。OpenImagesモデルからファインチューニングを行った場合、これらの作品を上回る結果を得ることができた。OpenImagesモデルは、Jahnら[^37]の結果を、FIDの点でほぼ11のマージンで上回った。図16では、COCO上でファインチューニングしたモデルの追加サンプルを示している。

### D.4 ImageNet上のクラス条件付き画像合成

表10は、FIDとInception score (IS)で測定したクラス条件付きLDMの結果である。LDM-8は、非常に競争力のある性能を達成するために、必要なパラメータと計算量を大幅に削減している（表18参照）。以前の研究と同様に、各ノイズスケールで分類器を訓練し、それでガイドすることでさらに性能を高めることができる（[C節](#c-画像ガイドの仕組み)を参照）。ピクセルベースの方法とは異なり、この分類器は潜在空間で非常に安価に訓練される。その他の定性的な結果については、図26と図27を参照されたい。

### D.5 サンプルの品質とV100Daysの比較（[4.1項](#41-知覚圧縮のトレードオフについて)の続き）

[4.1節](#41-知覚圧縮のトレードオフについて)でトレーニングの進行に伴うサンプルの品質の評価として、トレーニングステップの関数としてFIDとISスコアを報告した。もう一つの可能性は、V100日の使用リソースに対してこれらのメトリクスを報告することである。このような分析は、図17で追加的に提供され、定性的には同様の結果を示している。

### D.6 超解像

ピクセル空間におけるLDMと拡散モデルの比較のために、表5の分析を拡張し、同じステップ数で同等のパラメータ数で学習させた拡散モデルをLDMと比較する。この比較の結果は、表11の最後の2行に示されており、LDMがより良い性能を達成しながら、大幅に高速なサンプリングを可能にすることを実証している。定性的な比較は、ピクセル空間におけるLDMと拡散モデルの両方からのランダムなサンプルを示す図20に示されている。

#### D.6.1 LDM-BSR：多様な画像劣化を考慮した汎用SRモデル

LDM-SRの汎化性を評価するために、クラス条件付きImageNetモデル（[4.1節](#41-知覚圧縮のトレードオフについて)）から得た合成LDMサンプルとインターネットからクローリングした画像の両方に適用した。興味深いことに、[^72]のようにバイキュービックダウンサンプリングした条件付けのみで学習したLDM-SRは、この前処理を行わない画像にはうまく一般化しないことが観察された。したがって、カメラノイズ、圧縮アーチファクト、ぼかし、補間などの複雑な重ね合わせを含む可能性のある、幅広い実世界画像の超解像モデルを得るために、LDM-SRのバイキュービックダウンサンプリング演算を[^105]の劣化パイプラインで置き換える。BSR-劣化処理は、JPEG圧縮ノイズ、カメラセンサノイズ、ダウンサンプリング用の異なる画像補間、ガウスブラーカーネル、ガウスノイズをランダムな順序で画像に適用する劣化パイプラインである。[^105]のようにオリジナルのパラメータでbsr-degredation処理を使用すると、非常に強い劣化処理になることがわかった。私たちのアプリケーションには、より穏やかな劣化処理が適していると思われたため、bsr-degradationのパラメータを調整した（調整した劣化処理は、私たちのコードベース（[https://github.com/CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion)）に掲載されている）。図18は、LDM-SRとLDM-BSRを直接比較することで、このアプローチの有効性を示している。後者は、固定的な前処理に限定したモデルよりもはるかに鮮明な画像を生成し、実世界での応用に適している。LDM-BSRのさらなる結果をLSUN-cowで示したのが図19である。

## E 実装の詳細とハイパーパラメータ

### E.1 ハイパーパラメータ

表12、表13、表14、表15に、すべての学習済みLDMモデルのハイパーパラメータの概要を示す。

### E.2 実装内容

#### E.2.1 条件付きLDMのためのτθの実装

テキストから画像、レイアウトから画像（[4.3.1節](#431-ldm用トランスフォーマーエンコーダー)）合成の実験では、入力 $y$ をトークン化して処理し、出力 $\zeta:=τθ(y)$、ここで $\zeta\in \mathbb{R}^{M\times d_\tau}$ を生成するマスクなし変換器として条件 $\tau_\theta$ を実装している。より具体的には、変換器は、以下のように、大域的自己注目層、層正規化、および位置ワイズMLPからなるN個の変換器ブロックから実装される。

$$\begin{align*}\\
  \zeta\larr&TokEmb\left(y\right)+PosEmb\left(y\right)\tag{18}\\
  for\space i&=1,\cdots,N:\\
  \zeta_1&\larr LayerNorm\left(\zeta\right)\tag{19}\\
  \zeta_2&\larr MultiHeadSelfAttention\left(\zeta_1\right)+\zeta\tag{20}\\
  \zeta_3&\larr LayerNorm\left(\zeta_2\right)\tag{21}\\
  \zeta&\larr MLP\left(\zeta_3\right)+\zeta_2\tag{22}\\
  \zeta\larr&LayerNorm\left(\zeta\right)\tag{23}\\
\end{align*}$$

$\zeta$ が利用できるようになると、図3に描かれているように、コンディショニングはクロスアテンションメカニズムを介してUNetにマッピングされる。我々は、"ablated UNet"[^15]アーキテクチャを変更し、self-attention層を、(i)self-attention層、(ii)position-wise MLP、(iii)cross-attention層の交互層を持つ $T$ ブロックからなる浅い（マスクされない）トランスフォーマーと置き換える(表16)。なお、(ii)と(iii)がない場合、このアーキテクチャは「ablated UNet」と同等である。

時間ステップ $t$ の条件付けを追加することで $\tau_\theta$ の表現力を高めることは可能であるが、推論速度を低下させるため、この選択は追求しないことにする。この修正についてのより詳細な分析は、今後の研究に委ねる。

テキストから画像へのモデルには、一般に公開されている tokenizer[^99]に依存する。レイアウト-画像モデルは、バウンディングボックスの空間的位置を離散化し、各ボックスを $(l,b,c)$ タプルとして符号化する。ここで、$l$ は（離散的な）左上、$b$ は右下の位置を表す。クラス情報は $c$ に含まれる。上記の両タスクに対する $\tau_\theta$ のハイパーパラメータは表17、UNetのハイパーパラメータは表13を参照のこと。

なお、[4.1節](#41-知覚圧縮のトレードオフについて)で述べたようなクラス条件付きモデルもクロスアテンションによって実装されており、$\tau_\theta$ はクラス $y$ を $\zeta$ にマッピングする、次元数512の単一の学習可能な埋め込み層である。

#### E.2.2 インペインティング

[4.5節](#45-潜像拡散を利用したインペインティング)の画像インペインティングの実験では、合成マスクを生成するコード[^88]を使用した。Places[^108]の2kの検証用サンプルと30kのテスト用サンプルの固定セットを使用する。トレーニングでは、サイズ $256\times256$ のランダムクロップとサイズ $512\times512$ の評価クロップを使用する。これは、[^88]のトレーニングとテストのプロトコルに従ったもので、彼らの報告したメトリクスを再現している（表7の†を参照）。図21に $LDM-4,w/attn$, 図22に $LDM-4,w/o\space attn,big,w/ft$ の定性的な結果を追加で掲載した。

### E.3 評価内容

本節では、[4節](#4-実験)で示した実験の評価について、さらに詳しく説明する。

#### E.3.1 無条件およびクラス条件付き画像合成における定量的な結果

我々は、一般的な慣例に従い、表1および表10に示すFID-, Precision-, Recall-スコア [^29] [^50] を計算するための統計量を推定する。1および10は、我々のモデルから得た5万個のサンプルと、示した各データセットの全トレーニングセットに基づいている。FIDスコアの計算にはtorch-fidelityパッケージ[^60]を使用した。しかし、データ処理パイプラインが異なると結果が異なる可能性があるため[^64]、Dhariwal and Nichol [^15]が提供するスクリプトで我々のモデルも評価した。その結果、ImageNet と LSUN-Bedrooms データセットでは、7.76 (torch-fidelity) 対 7.77 (Nichol and Dhariwal) と 2.95 対 3.0 という微妙に異なるスコアが得られた以外は、ほぼ一致した結果が得られ ました。将来的には、サンプルの品質評価のための統一的な手順の重要性を強調する。PrecisionとRecallは、NicholとDhariwalが提供したスクリプトを使用しても計算されている。

#### E.3.2 テキストから画像への合成

[^66]の評価プロトコルに従い、表2のText-to-ImageモデルのFIDとInception Scoreを算出する。2は、生成されたサンプルをMS-COCOデータセット[^51]の検証セットからの30000サンプルと比較することによって計算される。FIDとInception Scoreはtorch-fidelityで計算される。

#### E.3.3 レイアウトから画像への合成

COCOデータセットにおける表9のLayout-to-Imageモデルのサンプル品質を評価するために、一般的な手法 [^37] [^87] [^89] に従って、COCO Segmentation Challenge splitの2048個の未拡張サンプルについてFIDスコアを計算した。より良い比較可能性を得るために、[^37]と全く同じサンプルを使用する。OpenImagesデータセットでは、同様に彼らのプロトコルに従い、検証セットから2048枚の中央で切り取られたテスト画像を使用する。

#### E.3.4 超解像

超解像モデルの評価は、[^72]で提案されたパイプラインに従ってImageNet上で行う。すなわち、256px未満の短いサイズの画像は削除する（学習と評価の両方で）。ImageNetでは、アンチエイリアスをかけたバイキュービック補間を用いて低解像度の画像を生成する。FIDはtorch-fidelity [^60]を用いて評価し、検証用分割でサンプルを作成する。FIDのスコアについては、さらに訓練分割で計算された参照特徴量と比較する（表5と表11を参照）。

#### E.3.5 効率性分析

効率化のため、図6、17、7にプロットしたサンプル品質メトリクスは、5k個のサンプルに基づいて計算した。したがって、結果は表1および表10に示されたものと異なる場合がある。すべてのモデルは、表13と表14で提供されるように、同等の数のパラメータを持っている。我々は、個々のモデルの学習率を、安定的に学習できるように最大化する。そのため、学習率は表13と表14を参照し、異なる実行の間でわずかに変化する。

#### E.3.6 ユーザースタディ

表4に示すユーザー調査の結果では、[^72]のプロトコールに従い、2-alternative force-choice paradigmを使用して、2つの異なるタスクに対する人間の好みスコアを評価した。タスク1では、被験者は、対応するグランドトゥルースの高解像度/非マスクバージョンの間の低解像度/マスク画像と、中央の画像を条件付けとして使用して生成された合成画像を見せた。超解像では、「2つの画像のうち、どちらが真ん中の低解像度画像のより良い高画質版か」を問うた。Inpaintingについては、「2つの画像のうち、どちらが真ん中の画像のよりリアルなインペイント領域を含んでいるか」と質問した。タスク2では、同様に低解像度/マスク版を人間に見せ、2つの競合する方法によって生成された2つの対応する画像から好みを尋ねた。[^72]と同様に、ヒトは3秒間画像を見てから回答した。

## F 計算機要件

表18では，使用した計算資源の詳細な分析を行い，CelebA-HQ，FFHQ，LSUN，ImageNetデータセットで我々のベストパフォーマーモデルと最近の最先端モデルとの比較を行った[^15]。また、NVIDIA A100 GPUですべてのモデルを訓練しているため、A100とV100の比較で2.2倍のスピードアップを仮定し、A100日分をV100日分に変換している[^74]。サンプルの品質を評価するため、報告されたデータセットのFIDスコアを追加で報告する。StyleGAN2[^42]やADM[^15]などの最先端手法の性能に迫る一方で、必要な計算資源を大幅に削減することができた。

## G オートエンコーダーモデルの詳細

我々は、パッチベースの識別器 $D_\psi$ が、原画と再構成画像 $\mathcal{D}(\mathcal{E}(x))$ を区別するように最適化されるように、[^23]に従って、全てのオートエンコーダーモデルを敵対的に訓練する。任意にスケーリングされた潜在空間を避けるために、正則化損失項 $L_{reg}$ を導入することにより、潜在 $z$ をゼロセンター化し、小さな分散を得るように正則化する。
(i)標準的な変分オートエンコーダ[^46] [^69]のように $q_\mathcal{E}(z|x)=\mathcal{N}(z;\mathcal{E}_\mu,\mathcal{E}_{\sigma^2})$ と標準正規分布 $\mathcal{N}(z;0,1)$ の間の低重みのカルバック-ライブラー項、, (ii) $|\mathcal{Z}|$ 異なる模範のコードブック学習によりベクトル量子化層で潜在空間を正規化する方法 [^96] の2種類の正規化方法について検討した。
忠実な再構成を得るために、両シナリオとも非常に小さな正則化しか使用しない、すなわち、$\mathbb{KL}$ 項を $10^{-6}$ 倍で重み付けするか、コードブックの次元数 $|\mathcal{Z}|$ を高くするかである。

自動符号化モデル $(\mathcal{E,D})$ を訓練する完全な目的は以下の通りである。

$$L_{Autoencoder}=\min_{\mathcal{E,D}}\max_\psi\left(L_{rec}\left(x,\mathcal{D}\left(\mathcal{E}\left(x\right)\right)\right)-L_{adv}\left(\mathcal{D}\left(\mathcal{E}\left(x\right)\right)\right)+\log D_\phi\left(x\right)+L_{reg}\left(x;\mathcal{E,D}\right)\right)\tag{25}$$

### 潜在空間における拡散モデルの学習

なお、学習した潜在空間上で拡散モデルを学習する場合、$p(z)$ または $p(z|y)$ を学習する際に、再び以下の2つのケースを区別する（[4.3節](#43-条件付き潜伏拡散)）。（i）KL規則化潜在空間に対して、$z=\mathcal{E}_\mu(x)+\mathcal{E}_\sigma(x)\cdotε=:\mathcal{E}(x)$、$ε〜\mathcal{N}(0,1)$ をサンプルする。潜在を再スケーリングする際、データ中の最初のバッチから成分ごとの分散

$$\hat{\sigma}^2=\frac{1}{bchw}\sum_{b,c,h,w}\left(z^{b,c,h,w}-\hat{\mu}\right)^2$$

を推定し、ここで、$\hat{\mu}=\frac{1}{bchw}\sum_{b,c,h,w}z^{b,c,h,w}$。$\mathcal{E}$ の出力は、再スケーリングされた潜在が単位標準偏差、すなわち $z\leftarrow\frac{z}{\hat{\sigma}}=\frac{\mathcal{E}(x)}{\hat{\sigma}}$ を持つようにスケーリングされる。 (ii) VQ正則化潜在空間の場合、量子化層の前に $z$ を抽出し、量子化演算をデコーダに吸収する、すなわち $\mathcal{D}$ の第1層として解釈することができる。

## H その他の定性的結果

最後に、ランドスケープモデル（図12、23、24、25）、クラス条件付きImageNetモデル（図26～27）、CelebA-HQ、FFHQ、LSUNデータセットに対する無条件モデル（図28～31）の定性的な結果を追加して示す。また、[4.5節](#45-潜像拡散を利用したインペインティング)のインペインティングモデルと同様に、[4.3.2節](#432-2562を超えるコンボリューショナルサンプリング)のセマンティックランドスケープモデルを $512^2$ 枚の画像で直接微調整し、図12と図23に定性的結果を示した。また、比較的に小さなデータセットで学習したモデルについては、VGG [^79]特徴空間における最近傍を図32〜34に追加で示す。

***

![より積極的でないダウンサンプリングによる達成可能な品質の上限の後押し](2023-06-02-11-27-54.png)

図1：**より積極的でないダウンサンプリングによる達成可能な品質の上限の後押し**
拡散モデルは空間データに対して優れた帰納的バイアスを提供するため、潜在空間における関連する生成モデルの重い空間ダウンサンプリングは必要ないが、適切な自動符号化モデルによってデータの次元を大幅に削減できる（第3節参照）。画像はDIV2K[^1]の検証セットで、$512^2$ pxで評価した。再構成FIDs [^29]とPSNRはImageNet-val[^12]で計算したもの。（表8参照）

![知覚的圧縮と意味的圧縮の図解](2023-06-02-11-45-51.png)

図2：**知覚的圧縮と意味的圧縮の図解**
デジタル画像のほとんどのビットは、知覚できない細部に対応している。DMは、責任損失項を最小化することにより、この意味的に無意味な情報を抑制することができるが、（学習時の）勾配と（学習と推論の）ニューラルネットワークのバックボーンは、依然としてすべてのピクセルで評価する必要があるため、余計な計算と不必要に高価な最適化と推論につながる。
我々は、効果的な生成モデルとして潜在拡散モデル（LDM）を提案し、知覚できない細部のみを除去するマイルドな圧縮段階を別に設ける。データおよび画像は[^30]より引用。

![LDMの構造](2023-06-02-14-24-39.png)

図3：**LDMの構造**
LDMの条件付けは、連結か、より一般的なクロスアテンションメカニズムで行う。3.3節参照

![図4](2023-06-09-13-33-49.png)

図4：CelebAHQ [^39], FFHQ [^41], LSUN-Churches [^102], LSUN-Bedrooms [^102], およびクラス条件付き ImageNet [^12] で学習したLDMのサンプル（それぞれ256 × 256の解像度）。拡大すると見やすくなる。その他のサンプルは、付録を参照のこと。

![図5](2023-06-09-13-40-28.png)

図5：LAION[^78]データベースで学習させたテキスト画像合成モデルLDM-8(KL)のユーザー定義テキストプロンプトのサンプル。200DDIMステップ、$\eta=1.0$で生成したサンプル。無条件ガイダンス[^32]を用い、$s=10.0$とした。

![図6](2023-06-09-13-36-14.png)

図6：ImageNetデータセットにおける2Mの訓練ステップにわたって、異なるダウンサンプリングファクター$f$を持つクラス条件付きLDMの訓練を分析する。ピクセルベースのLDM-1は、より大きなダウンサンプリング係数を持つモデル（LDM-{4-16}）と比較して、大幅に大きな学習時間を必要とする。LDM-32のように知覚的な圧縮が強すぎると、全体のサンプル品質が制限される。すべてのモデルは、同じ計算予算で単一のNVIDIA A100で学習している。結果は、100 DDIMステップ[^84]と$\kappa=0$を使用して得られた。

![図7](2023-06-09-13-42-39.png)

図7：CelebA-HQ（左）とImageNet（右）のデータセットで圧縮率を変化させたLDMを比較した。異なるマーカーは、DDIMを用いた{10, 20, 50, 100, 200}のサンプリングステップを、各線に沿って右から左へ示す。破線は200ステップのFIDスコアを示し、LDM- {4-8}の強力な性能を示す。FIDスコアは5000サンプルで評価した。すべてのモデルは、A100で500k（CelebA）/2M（ImageNet）ステップの学習を行った。

![図8](2023-06-09-13-50-47.png)

図8：COCO [^4]上のLDMによるレイアウトから画像への合成。4.3.1項参照。定量的な評価については、付録D.3.を参照。

![図9](2023-06-09-14-19-47.png)

図9：$256^2$の解像度で学習したLDMは、風景画像の意味合成のような空間的に条件付けられたタスクのために、より大きな解像度（ここでは512×1024）に一般化できる。[4.3.2節](#432-2562を超えるコンボリューショナルサンプリング) を参照。

![図10](2023-06-09-14-21-49.png)

図10：ImageNet 64→256の超解像をImageNet-Valで行った。LDM-SRはリアルなテクスチャの描画に有利だが、SR3はよりコヒーレントな微細構造の合成が可能。追加のサンプルとクロップアウトについては、付録を参照。SR3の結果は[^72]による。

![図11](2023-06-12-09-32-06.png)

図11：大きな$w/ft$のインペインティングモデルによるオブジェクト除去の定性的結果。より詳細な結果については、図22を参照。

![図12](2023-06-12-10-11-04.png)

図12：[4.3.2節](#432-2562を超えるコンボリューショナルサンプリング) のセマンティックランドスケープモデルによる畳み込みサンプル。$512^2$枚の画像で微調整された。

![図13](2023-06-12-10-12-49.png)

図13：分類器なしの拡散ガイダンスと[4.3.2節](#432-2562を超えるコンボリューショナルサンプリング)の畳み込みサンプリング戦略を組み合わせることで、我々の1.45Bパラメータのテキスト-イメージモデルは、モデルが学習したネイティブの$256^2$解像度よりも大きな画像のレンダリングに使用できるようになった。

![図14](2023-06-16-11-32-23.png)

図14：ランドスケープにおいて、無条件モデルを用いた畳み込みサンプリングは、均質かつ非干渉なグローバル構造をもたらすことがある（コラム2参照）。低解像度の画像で $L_2$ ガイドを行うと、コヒーレントなグローバル構造を再確立することができる。

![図15](2023-06-16-11-34-12.png)

図15：畳み込みサンプリングにおける潜在空間の再スケーリングの効果を説明するもので、ここでは風景の意味画像合成を対象としている。[4.3.2節](#432-2562を超えるコンボリューショナルサンプリング)と[D.1節](#d1-高解像度合成のための信号対雑音比の選択)を参照。

![図16](2023-06-16-11-35-40.png)

図16：OpenImages データセットで学習し、COCO データセットで微調整した、レイアウトから画像への合成に最適なモデル LDM-4 による、さらなるサンプル。100DDIM ステップ、$\eta=0$ で生成したサンプル。

![図17](2023-06-16-11-37-19.png)

図17：完成度を高めるため、ImageNet データセットにおけるクラス条件付き LDM の学習経過を、35V100 日の固定数で報告する。結果は100 DDIM ステップ[^84]と $\kappa=0$ で得られた。FID は効率上の理由から 5000 サンプルで計算された。

![図18](2023-06-16-11-39-48.png)

図18：LDM-BSR は、任意の入力に対して一般化し、クラス条件付きLDM（図4の画像）からのサンプルを $1024^2$ の解像度にアップスケーリングする、汎用アップサンプラーとして使用することが可能。一方、固定的な劣化処理（[4.4節](#44-潜在拡散による超解像)）を用いると、汎用性が損なわれる。

![図19](2023-06-16-11-41-59.png)

図19：LDM-BSRは任意の入力に一般化され、汎用のアップサンプラーとして、LSUN-Cows データセットのサンプルを $1024^2$ 分解能にアップスケーリングして使用することが可能。

![図20](2023-06-26-13-47-02.png)

図20：PixelspaceにおけるLDM-SRとベースライン拡散モデルの2つのランダムサンプルの質的超解像比較。同じ量のトレーニングステップを行った後、イメージネット検証セットで評価。

![表1](2023-06-09-13-46-19.png)

表1：無条件に画像合成を行うための評価指標。CelebA-HQの結果は[^43] [^63] [^100]から、FFHQは[^42] [^43]から再現した。†: $N-s$はDDIM[^84]サンプラーによる$N$回のサンプリングステップを指す。∗(注)$KL$正則化された潜在空間で学習した。追加の結果は補足にある。

![表2](2023-06-09-13-49-00.png)

表2：テキスト条件付き画像合成の評価について256×256サイズのMS-COCO[^51]データセットで評価：250個のDDIM [^84]ステップで、最新の拡散法[^59]や自己回帰法[^26]に匹敵する。自己回帰法[^26]と同等である。† / ∗ 数値は[^109]/[^26]による

![表3](2023-06-09-13-52-08.png)

表3：クラス条件付きImageNet LDMと、ImageNet上のクラス条件付き画像生成のための最近の最新手法との比較 [^12]. より詳細なベースラインとの比較はD.4、表10、およびF。 c.f.g.は[^32]で提案されたスケールsを用いた分類器不要のガイダンスを示す。

![表4](2023-06-09-14-23-34.png)

表4：タスク1：被験者は、真実の画像と生成された画像を見せられ、好みを尋ねられた。タスク2：被験者は2つの生成画像のどちらを選ぶか決定する必要があった。詳細はE.3.6に記載。

![表5](2023-06-09-14-24-55.png)

表5：ImageNet-Val.での×4アップスケーリング結果 ($256^2$); †： 検証用分割で計算されたFID特徴量，‡．FIDの特徴量はTrain分割で計算、∗： NVIDIA A100で評価。

![表6](2023-06-09-14-26-34.png)

表6：インペインティングの効率を評価する。†: GPUの設定/バッチサイズの違いによる図7からの乖離（付録参照）。

![表7](2023-06-12-09-33-35.png)

表7：Places[^108]のテスト画像からサイズ512×512の30kクロップでインペイントの性能を比較したもの。列$40-50\%$は、画像領域の40～50%をインペイントする必要がある難しい例で計算されたメトリクスを報告している。^†^[^88]で使用されたオリジナルのテストセットが入手できなかったため，我々のテストセットで再計算した。

![表8](2023-06-23-11-05-31.png)

表8：OpenImagesで学習したオートエンコーダズーをImageNet-Valで評価。†はアテンションフリーオートエンコーダを表す。

![表11](2023-06-26-13-42-13.png)

表11：ImageNet-Val($256^2$)による4倍拡大結果。†：検証スプリットで計算されたFID特徴。‡：訓練分割で計算されたFID特徴。また、LDM-4と同量の計算を受けたピクセル空間ベースラインも含む。最後の2行は15エポックの追加訓練を受けている。

![表18](2023-06-23-14-38-48.png)

表18：最新の生成モデルによる学習時の計算量と推論スループットの比較。V100日でのトレーニング中に計算、競合するメソッドの数は特に断りのない限り[^15]から引用。*:シングルNVIDIA A100で測定したスループット（サンプル/秒）。†:数字は[^15]から引用。‡:2,500万個の訓練例で学習されると仮定。††:R-FIDとImageNet検証セットの比較

[^1]:Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1122–1131. IEEE Computer Society, 2017.

[^2]:Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan, 2017.

[^3]:Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Int. Conf. Learn. Represent., 2019.

[^4]:Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1209–1218. Computer Vision Foundation / IEEE Computer Society, 2018.

[^5]:Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633–2650, 2021.

[^6]:Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 1691–1703. PMLR, 2020.

[^7]:Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In ICLR. OpenReview.net, 2021.

[^8]:Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolution. In NeurIPS, 2020.

[^9]:Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020.

[^10]:Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019.

[^11]:Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In ICLR (Poster). OpenReview.net, 2019.

[^12]:Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248–255. IEEE Computer Society, 2009.

[^13]:Emily Denton. Ethical considerations of generative ai. AI for Content Creation Workshop, CVPR, 2021.

[^14]:Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.

[^15]:Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. CoRR, abs/2105.05233, 2021.

[^16]:Sander Dieleman. Musings on typicality, 2020.

[^17]:Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-toimage generation via transformers. CoRR, abs/2105.13290, 2021.

[^18]:Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation, 2015.

[^19]:Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.

[^20]:Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Adv. Neural Inform. Process. Syst., pages 658–666, 2016.

[^21]:Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context with multi-nomial diffusion for autoregressive image synthesis. CoRR, abs/2108.08827, 2021.

[^22]:Patrick Esser, Robin Rombach, and Bjorn Ommer. A note on data biases in generative models. arXiv preprint arXiv:2012.02516, 2020.

[^23]:Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. CoRR, abs/2012.09841, 2020.

[^24]:Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and videotape: Deep fakes and free speech delusions. Md. L. Rev., 78:892, 2018.

[^25]:Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through languageimage encoders. ArXiv, abs/2106.14843, 2021.

[^26]:Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scenebased text-to-image generation with human priors. CoRR, abs/2203.13131, 2022.

[^27]:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, 2014.

[^28]:Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017.

[^29]:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst., pages 6626–6637, 2017.

[^30]:Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.

[^31]:Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. CoRR, abs/2106.15282, 2021.

[^32]:Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.

[^33]:Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 5967–5976. IEEE Computer Society, 2017.

[^34]:Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967–5976, 2017.

[^35]:Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Henaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: A general architecture for structured inputs &outputs. CoRR, abs/2107.14795, 2021.

[^36]:Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4651–4664. PMLR, 2021.

[^37]:Manuel Jahn, Robin Rombach, and Bjorn Ommer. High-resolution complex scene synthesis with transformers. CoRR, abs/2105.06458, 2021.

[^38]:Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, and Subbarao Kambhampati. Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses. arXiv preprint arXiv:2001.09528, 2020.

[^39]:Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017.

[^40]:Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4401–4410, 2019.

[^41]:T.Karras, S.Laine, and T.Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

[^42]:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. CoRR, abs/1912.04958, 2019.

[^43]:Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score matching model for unbounded data score. CoRR, abs/2106.05527, 2021.

[^44]:Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R.Garnett, editors, Advances in Neural Information Processing Systems, 2018.

[^45]:Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021.
[^46]:Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR, 2014.

[^47]:Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. CoRR, abs/2106.00132, 2021.

[^48]:Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In ICLR. OpenReview.net, 2021.
[^49]:Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R.R.Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari. The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. CoRR, abs/1811.00982, 2018.

[^50]:Tuomas Kynk ̈a ̈anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. CoRR, abs/1904.06991, 2019.

[^51]:Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. 6, 7, 27

[^52]:Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Aishan Liu, Dacheng Tao, and Edwin Hancock. Region-wise generative adversarial imageinpainting for large missing areas. ArXiv, abs/1909.12507, 2019.

[^53]:Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, JunYan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. CoRR, abs/2108.01073, 2021.

[^54]:Lars M. Mescheder. On the convergence properties of GAN training. CoRR, abs/1801.04406, 2018.

[^55]:Luke Metz, Ben Poole, David Pfau, and Jascha SohlDickstein. Unrolled generative adversarial networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.

[^56]:Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.

[^57]:Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. CoRR, abs/2103.16091, 2021.

[^58]:Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, and Mehran Ebrahimi. Edgeconnect: Generative image inpainting with adversarial edge learning. ArXiv, abs/1901.00212, 2019.

[^59]:Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. CoRR, abs/2112.10741, 2021.

[^60]:Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-fidelity performance metrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738.

[^61]:Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and JunYan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.

[^62]:Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and JunYan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.

[^63]:Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 823–832. Computer Vision Foundation / IEEE, 2021.

[^64]:Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv preprint arXiv:2104.11222, 2021.

[^65]:David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. CoRR, abs/2104.10350, 2021.

[^66]:Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021.

[^67]:Ali Razavi, A ̈aron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, pages 14837–14847, 2019.

[^68]:Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, 2016.

[^69]:Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML, 2014.

[^70]:Robin Rombach, Patrick Esser, and Bj ̈orn Ommer. Network-to-network translation with conditional invertible neural networks. In NeurIPS, 2020.

[^71]:Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In MICCAI (3), volume 9351 of Lecture Notes in Computer Science, pages 234–241. Springer, 2015.

[^72]:Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. CoRR, abs/2104.07636, 2021.

[^73]:Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. CoRR, abs/1701.05517, 2017.

[^74]:Dave Salvator. NVIDIA Developer Blog. [https://developer.nvidia.com/blog/getting-immediate-speedups-with-a100-tf32](https://developer.nvidia.com/blog/getting-immediate-speedups-with-a100-tf32), 2020.

[^75]:Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. CoRR, abs/2104.02600, 2021.

[^76]:Axel Sauer, Kashyap Chitta, Jens M ̈uller, and Andreas Geiger. Projected gans converge faster. CoRR, abs/2111.01007, 2021.

[^77]:Edgar Sch ̈onfeld, Bernt Schiele, and Anna Khoreva. A unet based discriminator for generative adversarial networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204–8213. Computer Vision Foundation / IEEE, 2020.

[^78]:Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion400m: Open dataset of clip-filtered 400 million image-text pairs, 2021.

[^79]:Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn. Represent., 2015.

[^80]:Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2C: diffusion-denoising models for few-shot conditional generation. CoRR, abs/2106.06819, 2021.

[^81]:Charlie Snell. Alien Dreams: An Emerging Art Scene. [https://ml.berkeley.edu/blog/posts/clip-art/](https://ml.berkeley.edu/blog/posts/clip-art/), 2021. [Online; accessed November-2021].

[^82]:Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015.

[^83]:Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.

[^84]:Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR. OpenReview.net,2021.

[^85]:Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. CoRR, abs/2011.13456, 2020.

[^86]:Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 13693–13696. AAAI Press, 2020.

[^87]:Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. CoRR, abs/2003.11571, 2020. 22, 27
[^88]:Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S. Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. ArXiv, abs/2109.07161, 2021.
[^89]:Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 2647–2655. AAAI Press, 2021.

[^90]:Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face does not exist... but it might be yours! identity leakage in generative models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1320–1328, 2021.

[^91]:Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521–1528. IEEE, 2011.

[^92]:Arash Vahdat and Jan Kautz. NVAE: A deep hierarchicalvariational autoencoder. In NeurIPS, 2020.

[^93]:Arash Vahdat, Karsten Kreis, and Jan Kautz. Scorebased generative modeling in latent space. CoRR, abs/2106.05931, 2021.

[^94]:Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, 2016.

[^95]:A ̈aron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016.

[^96]:A ̈aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, pages 6306–6315, 2017.

[^97]:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998–6008, 2017.

[^98]:Rivers Have Wings. Tweet on Classifier-freeguidance for autoregressive models. [https://twitter.com/RiversHaveWings/status/1478093658716966912](https://twitter.com/RiversHaveWings/status/1478093658716966912), 2022.

[^99]:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ́emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019.
[^100]:Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. VAEBM: A symbiosis between variational autoencoders and energy-based models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.

[^101]:Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021.

[^102]:Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015.

[^103]:Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan, 2021.

[^104]:Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Free-form image inpainting with gated convolution. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4470–4479, 2019.

[^105]:K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. ArXiv, abs/2103.14006, 2021.

[^106]:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.

[^107]:Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I-Chao Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. ArXiv, abs/2103.10428, 2021.

[^108]:Bolei Zhou, `Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:1452–1464, 2018.

[^109]:Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: towards language-free training for text-to-image generation. CoRR, abs/2111.13792, 2021.
