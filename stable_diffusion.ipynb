{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stable Diffusion** ğŸ¨\n",
    "*...using 'ğŸ§¨diffusers'*\n",
    "\n",
    "Stable Diffusionã¯ã€[CompVis](https://github.com/CompVis)ã€[Stability AI](https://stability.ai/)ã€[LAION](https://laion.ai/)ã®ç ”ç©¶è€…ã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒé–‹ç™ºã—ãŸtext-to-imageã®diffusion modelã§ã™ã€‚[LAION-5B](https://laion.ai/blog/laion-5b/)ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã®512x512ã®ç”»åƒã§å­¦ç¿’ã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯åœ§ç¸®ã•ã‚ŒãŸCLIP ViT-L/14ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã€æ–‡å­—å…¥åŠ›ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ä»¶ä»˜ã‘ã¾ã™ã€‚860Mã®UNetã¨123Mã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’æŒã¤ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ¯”è¼ƒçš„è»½é‡ã§ã€å¤šãã®å€‹äººå‘ã‘GPUã§å‹•ä½œã—ã¾ã™ã€‚è©³ã—ãã¯[model card](https://huggingface.co/CompVis/stable-diffusion)ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ğŸ¤—Hugging Face[ğŸ§¨Diffusers library](https://github.com/huggingface/diffusers)ã‚’ä½¿ã£ã¦ã€Stable Diffusionã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n",
    "\n",
    "ãã‚Œã§ã¯å§‹ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `StableDiffusionPipeline`ã®ä½¿ã„æ–¹\n",
    "\n",
    "StableDiffusionãŒã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã®ã‹èª¬æ˜ã™ã‚‹å‰ã«ã€å°‘ã—å‹•ã‹ã—ã¦ã¿ã¾ã—ã‚‡ã†ğŸ¤—ã€‚\n",
    "\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç°¡å˜ã«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã®æ¨è«–ã‚’ã™ã‚‹æ–¹æ³•ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‰æº–å‚™\n",
    "\n",
    "ã¯ã˜ã‚ã«ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚æ¨è«–ãŒã‚ˆã‚Šé€Ÿããªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€`diffusers`ã€`scipy`ã€`ftfy`ã€`transformers`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚`accelerate`ã¯ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã®çŸ­ç¸®ã«ç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install diffusers==0.11.1\n",
    "!python -m pip install transformers scipy ftfy accelerate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Diffusion Pipeline\n",
    "\n",
    "`StableDiffusionPipeline`ã¯end-to-endã®æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã‚’ç°¡å˜ã«ç”Ÿæˆã™ã‚‹ãŸã‚ã«ç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã¾ãšã€ãƒ¢ãƒ‡ãƒ«ä¸­ã®ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Stable Diffusion ãƒãƒ¼ã‚¸ãƒ§ãƒ³1.4([CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4))ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä»–ã«ã‚‚ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã®ã§ã€ãœã²ãŠè©¦ã—ãã ã•ã„ã€‚\n",
    "\n",
    "* [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
    "* [stabilityai/stable-diffusion-2-1-base](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\n",
    "* [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1) ã“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯768x768ã®ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ãŒã€ä»–ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯512x512ã§å‹•ä½œã—ã¾ã™ã€‚\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ID[CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)ã«åŠ ãˆã¦ã€`from_pretrained`ã«ã‚‚ç‰¹å®šã®`revision`ã¨`torch_dtype`ã‚’æ¸¡ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "ç„¡æ–™ç‰ˆã®Google Colabä¸Šã§ã‚‚Stable Diffusionã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã„ã®ã§ã€ç­åˆ¶åº¦ãƒ–ãƒ©ãƒ³ãƒã‹ã‚‰ã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ã€‚[`fp16`](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16)ã‚’ä½¿ç”¨ã—ã€`torch_dtype=torch.float16`ã‚’æ¸¡ã™ã“ã¨ã§ã€float16ç²¾åº¦ã®é‡ã¿ã‚’æœŸå¾…ã™ã‚‹ã‚ˆã†ã«ãƒ‡ã‚£ãƒ•ãƒ¥ãƒ¼ã‚¶ãƒ¼ã«ä¼ãˆã¾ã™ã€‚\n",
    "\n",
    "å¯èƒ½ãªé™ã‚Šé«˜ã„ç²¾åº¦ã‚’ç¢ºä¿ã—ãŸã„å ´åˆã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—ãˆã‚‹ã“ã¨ã‚’è¦šæ‚Ÿã®ã†ãˆã§ã€`torch_dtype=torch.float16`ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’GPUã«ç§»ã—ã¦ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã‚’é«˜é€ŸåŒ–ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a phoyograph of an astronaut riding a horse\"\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "image.save(f\"astronaut_rides_horse.png\")\n",
    "\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šè¨˜ã®ã‚»ãƒ«ã‚’è¤‡æ•°å›å®Ÿè¡Œã™ã‚‹ã¨ã€æ¯å›ç•°ãªã‚‹ç”»åƒãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚å‡ºåŠ›ãŒå¤‰åŒ–ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ããªã„å ´åˆã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«seedå€¤ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚åŒã˜seedã‚’ä½¿ã†ã“ã¨ã§ã€åŒã˜ç”»åƒãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(1024)\n",
    "\n",
    "image = pipe(prompt, generator=generator).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼•æ•°`num_inference_steps`ã‚’ä½¿ã£ã¦æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¸€èˆ¬çš„ã«ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå¤šã„ã»ã©çµæœã¯ã‚ˆããªã‚Šã¾ã™ã€‚Stable Diffusionã¯æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã®1ã¤ã§ã‚ã‚Šã€æ¯”è¼ƒçš„å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ã‚‚ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®50ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã‚‚ã—ã‚ˆã‚Šé€Ÿãçµæœã‚’å¾—ãŸã„ã®ã§ã‚ã‚Œã°ã€ã‚ˆã‚Šå°ã•ãªæ•°å­—ã‚’æŒ‡å®šã—ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã¯ã€å‰ã¨åŒã˜ã‚·ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€ã‚ˆã‚Šå°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ã™ã€‚é¦¬ã®é ­ã‚„ãƒ˜ãƒ«ãƒ¡ãƒƒãƒˆã®ã‚ˆã†ãªç´°éƒ¨ã¯ã€å‰ã®ç”»åƒã¨æ¯”ã¹ã¦æ˜ç¢ºã«è¡¨ç¾ã•ã‚Œã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(1024)\n",
    "\n",
    "image = pipe(prompt, num_inference_steps=15, generator=generator).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ«ã®ã‚‚ã†ä¸€ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯`guidance_scale`ã§ã™ã€‚ã“ã‚Œã¯ã€æ¡ä»¶ä»˜ä¿¡å·ï¼ˆä»Šå›ã¯ãƒ†ã‚­ã‚¹ãƒˆï¼‰åŠã³å…¨ä½“çš„ãªã‚µãƒ³ãƒ—ãƒ«ã®å“è³ªã¸ã®æº–æ‹ ã‚’é«˜ã‚ã‚‹ãŸã‚ã®æ–¹æ³•ã§ã™ã€‚ç°¡å˜ã«è¨€ã†ã¨ã€åˆ†é¡æ°ã‚’ä½¿ã‚ãªã„ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¯ã€ã‚ˆã‚Šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒãƒƒãƒã™ã‚‹ã‚ˆã†ã«ç”Ÿæˆã‚’å¼·åˆ¶ã—ã¾ã™ã€‚`7`ã‚„`8.5`ã®ã‚ˆã†ãªæ•°å­—ã¯è‰¯ã„çµæœã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ãŒã€éå¸¸ã«å¤§ããªæ•°å­—ã‚’ä½¿ç”¨ã—ãŸå ´åˆã€ç”»åƒã¯ã‚ˆãè¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€å¤šæ§˜æ€§ã«æ¬ ã‘ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŠ€è¡“çš„ãªè©³ç´°ã«ã¤ã„ã¦ã¯ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®[æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³](https://colab.research.google.com/drive/1ALXuCM5iNnJDNW5vqBm5lCtUQtZJHN2f?authuser=1#scrollTo=UZp-ynZLrS-S)ã§èª¬æ˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦è¤‡æ•°ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°å›ç¹°ã‚Šè¿”ã—ãŸãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã ã‘ã§ã™ã€‚å…ˆã»ã©ä½¿ã£ãŸæ–‡å­—åˆ—ã®ä»£ã‚ã‚Šã«ã€ã“ã®ãƒªã‚¹ãƒˆã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«é€ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¾ãšã€é–¢æ•°ã®ã‚°ãƒªãƒƒãƒ‰ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨`image_grid`é–¢æ•°ãŒä½œæˆã•ã‚Œã¾ã™ã€‚ã©ã®ã‚ˆã†ã«è¡Œã‚ã‚Œã¦ã„ã‚‹ã‹èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯ã‚³ãƒ¼ãƒ‰ã‚’ç²¾èª­ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€3ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã™ã‚Œã°ã€ã‚°ãƒªãƒƒãƒ‰ç”»åƒãŒç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 3\n",
    "prompt = [\"a photograph of an astronaut riding a horse\"] * num_images\n",
    "\n",
    "images = pipe(prompt).images\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=3)\n",
    "grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n Ã— m`ã®ç”»åƒã‚’å‡ºåŠ›ã™ã‚‹ã«ã¯æ¬¡ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = 3\n",
    "num_rows = 4\n",
    "\n",
    "prompt = [\"a photograph of an astronaut riding a horse\"] * num_cols\n",
    "\n",
    "all_images = []\n",
    "for i in range(num_rows):\n",
    "    images = pipe(prompt).images\n",
    "    all_images.extend(images)\n",
    "\n",
    "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
    "grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­£æ–¹å½¢ã§ãªã„ç”»åƒã®ç”Ÿæˆ\n",
    "\n",
    "Stable Diffusionã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§`512x512`ãƒ”ã‚¯ã‚»ãƒ«ã®ç”»åƒã‚’ä½œæˆã—ã¾ã™ã€‚ã—ã‹ã—ã€`height`ã‚„`width`ã®å¼•æ•°ã‚’ä½¿ã£ã¦ã€é•·æ–¹å½¢ã®ç”»åƒã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
    "\n",
    "ä»¥ä¸‹ã¯ã€é©åˆ‡ãªã‚µã‚¤ã‚ºã‚’é¸æŠã™ã‚‹ãŸã‚ã®ã„ãã¤ã‹ã®æ¨å¥¨äº‹é …ã§ã™ã€‚\n",
    "- `height`ã¨`width`ãŒã¨ã‚‚ã«`8`ã®å€æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "- 512ã‚’ä¸‹å›ã‚‹ã¨ã€ä½å“è³ªã®ç”»åƒã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "- ç¸¦æ¨ªå…±ã«512ã‚’è¶…ãˆã‚‹ã¨ã€ç”»åƒé ˜åŸŸãŒç¹°ã‚Šè¿”ã•ã‚Œã¾ã™ï¼ˆå…¨ä½“çš„ãªã¾ã¨ã¾ã‚ŠãŒå¤±ã‚ã‚Œã¾ã™ï¼‰ã€‚\n",
    "- é•·æ–¹å½¢ã®ç”»åƒã‚’ä½œæˆã™ã‚‹ã¨ãã¯ã€1ã¤ã®æ¬¡å…ƒã«`512`ã‚’ã€ã‚‚ã†1ã¤ã®æ¬¡å…ƒã«ãã‚Œã‚ˆã‚Šã‚‚å¤§ããªå€¤ã‚’ä½¿ç”¨ã™ã‚‹ã®ãŒåŠ¹æœçš„ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse\"\n",
    "\n",
    "image = pipe(prompt, height=512, width=768).images[0]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stable Diffusionã¨ã¯\n",
    "\n",
    "ã•ã¦ã€Stable DiffusionğŸ‘©â€ğŸ“ã®è«–ç†çš„ãªéƒ¨åˆ†ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "Stable Diffusionã¯ã€[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)ã§ææ¡ˆã•ã‚ŒãŸ**Latent Diffusion**ã¨ã„ã†ç‰¹æ®ŠãªDiffusionãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€èˆ¬çš„ãªDiffusionãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã‚’æ®µéšçš„ã«é™¤å»ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã•ã‚ŒãŸæ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã§ã€ç”»åƒãªã©ã®ç›®çš„ã®ã‚µãƒ³ãƒ—ãƒ«ã«åˆ°é”ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªä»•çµ„ã¿ã«ã¤ã„ã¦ã¯ã€ã“ã¡ã‚‰ã®[Colab](https://colab/research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
    "\n",
    "Diffusionãƒ¢ãƒ‡ãƒ«ã¯ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã«ãŠã„ã¦ã€æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€Diffusionãƒ¢ãƒ‡ãƒ«ã®æ¬ ç‚¹ã¯ã€é€†ãƒã‚¤ã‚ºå‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ã§ã™ã€‚ã¾ãŸã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§å‹•ä½œã™ã‚‹ãŸã‚ã€å¤šãã®ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã—ã€é«˜è§£åƒåº¦ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹å ´åˆã«ã¯ä¸å½“ã«é«˜ä¾¡ã«ãªã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ã•ã‚‰ã«æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯å›°é›£ã§ã™ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Latent Diffusionã¯ã€å®Ÿéš›ã«ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã‚’ä½¿ç”¨ã™ã‚‹ä»£ã‚ã‚Šã«ã€ä½æ¬¡å…ƒã®æ½œåœ¨ç©ºé–“ä¸Šã§æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ã®è¤‡é›‘ã•ã‚’è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚ŒãŒæ¨™æº–çš„ãªDiffusionãƒ¢ãƒ‡ãƒ«ã¨Latent Diffusionãƒ¢ãƒ‡ãƒ«ã®ä¸»ãªé•ã„ã§ã™ã€‚**Latent Diffusionã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒã®æ½œä¼ï¼ˆåœ§ç¸®ï¼‰è¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã•ã‚Œã¾ã™ã€‚**\n",
    "\n",
    "Latent Diffusionã«ã¤ã„ã¦ã¯ã€3ã¤ã®ä¸»ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "1. ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆVAEï¼‰\n",
    "2. [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq)\n",
    "3. ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆ[CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)ãªã©ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "scheduler = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                         # default width of Stable Diffusion\n",
    "\n",
    "num_inference_steps = 100            # Number of denoising steps\n",
    "\n",
    "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "\n",
    "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn(\n",
    "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ")\n",
    "latents = latents.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = latents * scheduler.init_noise_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and decode the image latents with vae\n",
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
