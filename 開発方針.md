# R05 卒業研究 開発方針

5年 情報・ソフトウェア系 29番
f19135 本田涼大

## 背景

Stable Diffusion は潜在拡散モデルを用いて入力されたテキストプロンプトから画像を出力するものである。拡張機能などの開発が盛んであり、例えば、テキストプロンプトと同時に画像を入力して出力結果を制御する ControlNet などが開発されている。

AnimateDiff は Stable Diffusion を動画生成モデルに拡張したものであり、テキストプロンプトで指定した内容の動画を生成することができる。しかし、テキストプロンプトで指定できるのはあくまで人物の性別・人種や衣装、構図、背景などの状況に限られ、動画内の人物に意図した動作をさせることは難しい。

## 目的

出力動画内の人物の動作を、人間のモーションの動画を入力することで制御することを目的とする。

## 関連研究

Stable Diffusion

潜在拡散モデルを用いて

AnimateDiff

Stable Diffusion にモーショーンモジュールを取り付け、動きの情報を学習させて連続画像の出力を可能にする。

ControlNet

Stable Diffusion 内の凍結された U-net アーキテクチャに ControlNet アーキテクチャを接続し、出力画像の内容を意図したものに制御する。

## 提案手法

AnimateDiff での動画生成に ControlNet の画像内容制御機能を連結し、動画内容の制御を図る。

## 現在の課題

使用する LoRA それぞれに対するよりよいプロンプトの傾向を調査し、使用するすべてのLoRAに対して同等に高水準な結果を出せるようにする。

## 今後の予定
