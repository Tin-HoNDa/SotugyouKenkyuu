\documentclass[10pt]{article}
\usepackage[japanese]{babel}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\title{Text-to-Videoモデルを用いて生成された動画に対する定量的評価指標の検討とその評価}
\author{本田涼大}
\date{February 2024}

\begin{document}
\maketitle

\begin{abstract}
    近年動画像生成分野が目覚ましい発展を見せており，オープンソースかつ個人のコンピュータでも十分に利用できるほど軽量な，新たなモデルやLoRAが次々に登場している．しかし，動画像生成モデルの出力する動画像に対する評価指標に関する研究はモデルの研究と比較して殆ど行われてきていない．特に，Text-to-Videoモデルにおけるプロンプトと生成動画像の関連性に対する評価指標は既存手法ではGround-truthとの十分な相関が示されていない．そこで，プロンプトと生成動画像の関連性に対する評価指標として動画像キャプショニングモデルと言語埋込みモデルを用いた手法の有効性を検証する．実験の結果，提案手法とGround-truthとの高い相関は確認されず，既存手法であるCLIPScoreと比較しても大きな差がないことが分かった．
\end{abstract}

\section{はじめに}

昨今，生成 AI 分野が大きな発展を見せている．画像生成モデルである StableDiffusion \cite{rombach2022high} などのモデルが様々な分野で活躍している．特に動画像生成分野においては，StableDiffusion をもとに開発された Text-to-Video (以下 T2V)モデルであるAnimateDiff \cite{guo2023animatediff} や，イラストや写真内に写る人物に指定した動きをさせることができる Animate Anyone \cite{hu2023animate} などの高性能なモデルが次々に登場している．

一方，動画像生成分野においては評価指標の研究はそれほど盛んではない．T2V モデルに関するどの論文も定性的な評価のみを掲載し，定量的な評価については言及しないことが殆どである．数少ない T2V モデルに対する評価指標の1つである EvalCrafter \cite{liu2023evalcrafter} では動画ぞ追う生成モデルに対する評価を「映像の品質」「プロンプトと動画の関連性」「動き方の自然さ」「隣接フレーム間の整合性」「美的評価」の5つの観点から行っているが，いずれも人間の評価から生成された Ground-truth との相関はあまり高くなく，十分な性能であるとは言えない．特に「プロンプトと動画の関係性」に関しては，現在 CLIPScore \cite{hessel2021clipscore} が主に使われているが， Ground-truthとの相関が最も低く，十分な性能があるとはいえない．

本稿では，EvalCrafter において最も Ground-truth との相関が低かった分野である「プロンプトと動画の関係性」を評価する指標として，動画像キャプションモデルを用いた評価手法の有効性を評価する．

\section{提案手法}

T2V モデルはプロンプトと呼ばれる入力指示文からに従い動画を生成する．動画キャプションモデルは動画を元に説明文を作成する．此方には幾つか評価指標があり，高精度でキャプションが作成できることがわかっている．提案手法では，動画に対し生成したキャプションがプロンプトと意味的にどれだけ近いのかを評価することでプロンプトへの動画の追従度を評価する．

\section{関連研究}
\subsection{T2Vモデル}

本稿では AnimateDiff \cite{guo2023animatediff} を T2V モデルとして選択した．AnimateDiff は Text-to-Image (以下 T2I) モデルである StableDiffusion を動画像出力のために拡張したモデルである．

StableDiffusion \cite{rombach2022high} は潜在拡散モデルを用いて入力であるプロンプトから画像を出力する T2I モデルである．モデルは

AnimateDiff\cite{guo2023animatediff}

\subsection{CLIPScore}

CLIPScore\cite{hessel2021clipscore}

\subsection{キャプション生成モデル}

キャプション生成モデルは一般に動画像を処理するエンコーダ部とキャプションへと変換するデコーダ部からなる．今回使用したモデルはエンコーダ部に Vision Transformer \cite{dosovitskiy2020vit} を用いているもの \cite{vit+gpt2} と TimeSformer \cite{Bertasius2021tsf} を用いているもの \cite{tsf+gpt2} の2つである．デコーダ部は GPT2 \cite{radford2019gpt2} で共通している．

\subsection{言語埋込みモデル}

BERT \cite{devlin2018bert}

E5 \cite{wang2022e5}

\section{実験}

提案手法の有効性を評価するために，AnimateDiff を用いて生成した動画像31本に対し，提案手法により出力したスコアとアンケートの結果から得た Ground-truth の相関を見る．また，従来手法として CLIPScore \cite{hessel2021clipscore} を用いた場合の Ground-truth との相関も算出する．

実験に使用するプロンプトと動画像のペアを作成する．プロンプトは EvalCrafter \cite{liu2023evalcrafter} のリポジトリに置かれている700のプロンプトの中から無作為に抽出した．動画像生成にはAnimateDiff \cite{guo2023animatediff} を使用した．このとき，LoRA は AnimateDiff の Github リポジトリ内に存在する bash ファイルからダウンロードできる ToonYou，Lyriel，RcnzCartoon，MajicMix，RealisticVision，Tusun，FilmVelvia，GhibliBackground の8つを使用した．生成された動画はすべて秒間8フレーム，全体12フレームの2秒の動画で，解像度は512$\times$512である．

アンケートでは，生成した動画像とそのプロンプトに対して人間による評価を行った． 31組の動画像とプロンプトのペアに対し，被験者21人に「1. 動画に対する簡単な説明」「2. 動画像がプロンプトに従っているかの5段階評価」「3. 動画像の総合的な品質の5段階評価」を質問した．ただし，被験者が回答について相談などすることがないよう，対面にてアンケートを行った．アンケートは Microsoft Forms により作成した．質問1に回答する前に質問2に記述されているプロンプトが目に入らないように間に改ページを挟み，すべての回答欄を必須フィールドにしている．

次に，生成された動画像に対してキャプションを生成する．動画キャプションモデルの性能比較のため，キャプション生成は ViT+GPT2 \cite{vit+gpt2} と TSF+GPT2 \cite{tsf+gpt2} の2つのモデルを用いて行った．

最後に各動画像のプロンプトと生成されたキャプションを言語埋込みモデルを用いてコサイン類似度を算出する．パラメータ $\theta$ を持つ文書埋込みモデルを$\rm{emb}_\theta(\cdot)$，プロンプトの埋込みを$p$，キャプションの埋込みを$c$とするとき，例えばプロンプトの埋込みは$\rm{emb}_\theta(p)$で表される．生成された動画のプロンプトへの追従度を以下のように定める．

$$\cos(p,c;\theta)=\frac{\rm{emb}_\theta(p)\cdot \rm{emb}_\theta(c)}{\|\rm{emb}_\theta(p)\|\|\rm{emb}_\theta(c)\|}$$

\section{結果}

\begin{table}[htbp]
    \centering
    \begin{tabular}{l|c|l|rr}
        モデル名 & 入力 & 言語埋込みモデル & Spearman`s $\rho$ & Kendall's $\phi$ \\
        \hline\hline
        \begin{tabular}{l}CLIPScore \cite{hessel2021clipscore}\end{tabular}&動画&-&-0.1881&-0.1306\\
        \hline
        \multirow{2}{*}{\begin{tabular}{l}TimeSformer-GPT2\\Video Captioning \cite{tsf+gpt2}\end{tabular}}&\multirow{2}{*}{動画}&E5 \cite{wang2022e5}&0.1356&0.1088\\\cline{3-5}
        &&BERT \cite{devlin2018bert}&0.2176&0.1611\\
        \hline
        \multirow{8}{*}{\begin{tabular}{l}ViT-GPT2 Image\\Captioning \cite{vit+gpt2}\end{tabular}}&\multirow{2}{*}{動画}&E5&-0.2717&-0.2046\\\cline{3-5}
        &&BERT&-0.2556&-0.1828\\\cline{2-5}
        &\multirow{2}{*}{画像(平均)}&E5&-0.2557&-0.1915\\\cline{3-5}
        &&BERT&-0.2981&-0.1959\\\cline{2-5}
        &\multirow{2}{*}{画像(最高)}&E5&-0.1425&-0.1132\\\cline{3-5}
        &&BERT&-0.1786&-0.1349\\\cline{2-5}
        &\multirow{2}{*}{画像(最低)}&E5&\textbf{-0.3819}&\textbf{-0.2742}\\\cline{3-5}
        &&BERT&-0.3569&-0.2655\\
        \hline
        \multirow{8}{*}{\begin{tabular}{l}BLIP \cite{li2022blip}\end{tabular}}&\multirow{2}{*}{動画}&E5&-0.3668&-0.2481\\\cline{3-5}
        &&BERT&-0.0020&-0.0261\\\cline{2-5}
        &\multirow{2}{*}{画像(平均)}&E5&-0.3722&-0.2307\\\cline{3-5}
        &&BERT&-0.0854&-0.0653\\\cline{2-5}
        &\multirow{2}{*}{画像(最高)}&E5&-0.2198&-0.1524\\\cline{3-5}
        &&BERT&-0.0260&-0.0260\\\cline{2-5}
        &\multirow{2}{*}{画像(最低)}&E5&-0.3347&-0.2176\\\cline{3-5}
        &&BERT&-0.0166&-0.0261\\
    \end{tabular}
    \caption{\textbf{実験結果}．「入力」列にはそのモデルに動画形式で入力したか画像形式で入力したかを記述している．画像形式で入力した場合，各画像のスコアの平均・最高値・最低値を記録した場合でそれぞれ場合分けしている．}
    \label{tab:result}
\end{table}

実験結果を表\ref{tab:result}に示す．ほとんどの組み合わせにおいて無相関または負の弱い相関を記録した． ViT-GPT2 Image Captioning モデルに1フレームずつ入力し，その最低値をその動画像のスコアとしたときの記録が Ground-truth との相関係数の絶対値がスピアマン相関・ケンドール相関共に最も高い結果となった．

\section{考察}

Ground-truth との高い相関が得られなかった理由として，以下のことが考えられる．

\begin{quote}\begin{itemize}
    \item プロンプトに含まれることの多い画風やが書くなどの情報( \textit{e.g.} style of Hokusai, close-up ) や固有名詞( \textit{e.g.} Angelina Jolie, Darth Vader)をキャプション生成の時点で復元できず，比較の際にスコアが下がっている．
    \item キャプションモデル若しくは言語埋込みモデルの性能が不十分である．
    \item アンケートの回答者の年齢層及び所属が偏っていることが Ground-truth に何らかの偏りを生み出している．
\end{itemize}\end{quote}

今後の研究では上記の考察を検証するために，以下のことを実施し，プロンプトと生成動画像の関連性の評価に関する検討を続けていきたい．

\begin{quote}\begin{itemize}
    \item プロンプトから画風や画角に関する記述，および固有名詞を排除した状態で実験を行う．
    \item キャプション及び言語埋込みモデルについて，Ground-truth との比較により性能の評価を行う．
    \item 幅広い年齢層及び所属の人間からアンケートをとり，今回の実験の結果と比較する．
\end{itemize}\end{quote}



\bibliographystyle{alpha}
\bibliography{bib}

\end{document}
