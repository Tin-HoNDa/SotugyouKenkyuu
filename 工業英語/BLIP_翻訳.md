# BLIP:統一的な視覚言語理解と生成のためのブートストラップ言語画像事前学習

## Abstract

視覚言語事前訓練（VLP）は、多くの視覚言語タスクのパフォーマンスを向上させてきた。しかし、既存の訓練済みモデルのほとんどは、理解ベースのタスクか生成ベースのタスクのどちらかにしか秀でていない。さらに、性能の向上は、ウェブから収集したノイズの多い画像とテキストのペアでデータセットをスケールアップすることによって達成された。本論文では、視覚言語理解タスクと視覚言語生成タスクの両方に柔軟に移行できる新しいVLPフレームワークであるBLIPを提案する。BLIPは、キャプション作成者が合成キャプションを生成し、フィルタがノイズを除去するというブートストラップによって、ノイズの多いウェブデータを効果的に利用する。画像-テキスト検索(平均想起率+2.7%)、画像キャプション付け(CIDEr+2.8%)、VQA(VQAスコア+1.6%)など、様々な視覚言語タスクにおいて最先端の結果を達成している。BLIPはまた、ゼロショット方式でビデオ言語タスクに直接移行した場合にも、強力な汎化能力を示す。コード、モデル、データセットが公開される。

## 1 はじめに

## 2 関連研究

## 3 手法

我々は、ノイズの多い画像とテキストのペアから学習するための統一的なVLPフレームワークであるBLIPを提案する。このセクションでは、まず新しいモデル・アーキテクチャMEDとその事前学習の目的を紹介し、次にデータセットのブートストラッピングのためのCapFiltについて説明する。

### 1 モデルアーキテクチャ

我々は、グローバルな画像特徴を表現するために[CLS]トークンを追加した画像エンコーダとして視覚変換器（Dosovitskiy et al, 2021）を採用している。このエンコーダは、入力画像をパッチに分割し、それらをエンベッディングのシーケンスとして符号化する。視覚的特徴抽出のために事前に訓練された物体検出器を使用する場合（Chenら、2020）と比較して、ViTを使用することは、より計算が容易であり、より最近の手法で採用されている（Liら、2021a; Kimら、2021）。



## 4 実験と議論

## 5 先端技術との比較

## 6 追加アブレーション研究

## 7 終わりに
