@inproceedings{rombach2022high,
    title={High-resolution image synthesis with latent diffusion models},
    author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={10684--10695},
    year={2022}
}
@article{guo2023animatediff,
    title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
    author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Wang, Yaohui and Qiao, Yu and Lin, Dahua and Dai, Bo},
    journal={arXiv preprint arXiv:2307.04725},
    year={2023}
}
@article{hu2023animate,
    title={Animate anyone: Consistent and controllable image-to-video synthesis for character animation},
    author={Hu, Li and Gao, Xin and Zhang, Peng and Sun, Ke and Zhang, Bang and Bo, Liefeng},
    journal={arXiv preprint arXiv:2311.17117},
    year={2023}
}
@article{liu2023evalcrafter,
    title={Evalcrafter: Benchmarking and evaluating large video generation models},
    author={Liu, Yaofang and Cun, Xiaodong and Liu, Xuebo and Wang, Xintao and Zhang, Yong and Chen, Haoxin and Liu, Yang and Zeng, Tieyong and Chan, Raymond and Shan, Ying},
    journal={arXiv preprint arXiv:2310.11440},
    year={2023}
}
@article{dosovitskiy2020vit,
    title={An image is worth 16x16 words: Transformers for image recognition at scale},
    author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
    journal={arXiv preprint arXiv:2010.11929},
    year={2020}
}
@inproceedings{bertasius2021tsf,
    title={Is space-time attention all you need for video understanding?},
    author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
    booktitle={ICML},
    volume={2},
    number={3},
    pages={4},
    year={2021}
}
@article{radford2019gpt2,
    title={Language models are unsupervised multitask learners},
    author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
    journal={OpenAI blog},
    volume={1},
    number={8},
    pages={9},
    year={2019}
}
@misc{vit+gpt2,
    author={NLP Connect},
    title={vit-gpt2-image-captioning},
    note={https://huggingface.co/nlpconnect/vit-gpt2-image-captioning},
    year={2023}
}
@misc{tsf+gpt2,
    author={Caelen Wang},
    title={TimeSformer-GPT2 Video Captioning},
    note={https://huggingface.co/Neleac/timesformer-gpt2-video-captioning},
    year={2022}
}
@article{hessel2021clipscore,
    author={Jack Hessel and Ari Holtzman and Maxwell Forbes and Ronan Le Bras and Yejin Choi},
    title={CLIPScore: A Reference-free Evaluation Metric for Image Captioning},
    journal={arXiv preprint arXiv:2104.08718},
    year={2021} 
}
@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
    editor = "Burstein, Jill and Doran, Christy and Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{wang2022e5,
    author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
    title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
    journal={arXiv preprint arXiv:2212.03533},
    year={2022}
}
@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}
@inproceedings{radford2021clip,
    title={Learning transferable visual models from natural language supervision},
    author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
    booktitle={International conference on machine learning},
    pages={8748--8763},
    year={2021},
    organization={PMLR}
}
@article{vaswani2017transformer,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}
@article{achiam2023gpt4,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}