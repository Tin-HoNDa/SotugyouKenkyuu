{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, AutoModel, CLIPModel\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpconnect/vit-gpt2-image-captioning were not used when initializing VisionEncoderDecoderModel: ['decoder.transformer.h.11.attn.bias', 'decoder.transformer.h.4.attn.bias', 'decoder.transformer.h.0.attn.masked_bias', 'decoder.transformer.h.6.crossattention.bias', 'decoder.transformer.h.5.attn.bias', 'decoder.transformer.h.2.crossattention.bias', 'decoder.transformer.h.5.attn.masked_bias', 'decoder.transformer.h.11.attn.masked_bias', 'decoder.transformer.h.0.attn.bias', 'decoder.transformer.h.7.attn.masked_bias', 'decoder.transformer.h.1.attn.bias', 'decoder.transformer.h.9.attn.masked_bias', 'decoder.transformer.h.9.attn.bias', 'decoder.transformer.h.1.crossattention.bias', 'decoder.transformer.h.3.crossattention.masked_bias', 'decoder.transformer.h.7.crossattention.bias', 'decoder.transformer.h.10.attn.masked_bias', 'decoder.transformer.h.6.crossattention.masked_bias', 'decoder.transformer.h.2.attn.masked_bias', 'decoder.transformer.h.6.attn.masked_bias', 'decoder.transformer.h.11.crossattention.masked_bias', 'decoder.transformer.h.2.crossattention.masked_bias', 'decoder.transformer.h.4.crossattention.masked_bias', 'decoder.transformer.h.5.crossattention.bias', 'decoder.transformer.h.0.crossattention.masked_bias', 'decoder.transformer.h.7.crossattention.masked_bias', 'decoder.transformer.h.9.crossattention.masked_bias', 'decoder.transformer.h.5.crossattention.masked_bias', 'decoder.transformer.h.3.attn.masked_bias', 'decoder.transformer.h.8.attn.masked_bias', 'decoder.transformer.h.9.crossattention.bias', 'decoder.transformer.h.10.crossattention.bias', 'decoder.transformer.h.1.crossattention.masked_bias', 'decoder.transformer.h.7.attn.bias', 'decoder.transformer.h.8.crossattention.bias', 'decoder.transformer.h.10.attn.bias', 'decoder.transformer.h.8.attn.bias', 'decoder.transformer.h.8.crossattention.masked_bias', 'decoder.transformer.h.11.crossattention.bias', 'decoder.transformer.h.3.crossattention.bias', 'decoder.transformer.h.1.attn.masked_bias', 'decoder.transformer.h.4.attn.masked_bias', 'decoder.transformer.h.3.attn.bias', 'decoder.transformer.h.4.crossattention.bias', 'decoder.transformer.h.0.crossattention.bias', 'decoder.transformer.h.10.crossattention.masked_bias', 'decoder.transformer.h.2.attn.bias', 'decoder.transformer.h.6.attn.bias']\n",
      "- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(l):\n",
    "    #l_min = min(l)\n",
    "    #l_max = max(l)\n",
    "    l_min = 1\n",
    "    l_max = 5\n",
    "    return [((i - l_min) / (l_max - l_min)) for i in l]\n",
    "\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_videos = \"splitted/enquete/\"\n",
    "dir_prompts = \"./アンケート/prompts/\"\n",
    "\n",
    "video_paths = [os.path.join(dir_videos, x) for x in os.listdir(dir_videos)]\n",
    "prompt_paths = [os.path.join(dir_prompts, x) for x in os.listdir(dir_prompts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_num:496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [19:02<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "test_num = len(video_paths)\n",
    "print(f\"test_num:{test_num}\")\n",
    "\n",
    "for i in tqdm(range(len(video_paths))):\n",
    "    video_path = video_paths[i]\n",
    "    sequence = predict_step([video_path])\n",
    "    sequences.append(sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-base').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:41<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "min_outputs = []\n",
    "#max_outputs = []\n",
    "#mean_outputs = []\n",
    "\n",
    "min_captions = []\n",
    "\n",
    "for i in tqdm(range(len(prompt_paths))):\n",
    "    prompt_path = prompt_paths[i]\n",
    "    scores = []\n",
    "\n",
    "    for j in range(16):\n",
    "        sequence = sequences[i * 16 + j]\n",
    "\n",
    "        sequence = 'query: ' + str(sequence)\n",
    "        text = 'passage: ' + str(read_text_file(prompt_path))\n",
    "\n",
    "        input_texts = [sequence, sequence, text, text]\n",
    "        batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        output = model(**batch_dict)\n",
    "        embeddings = average_pool(output.last_hidden_state, batch_dict['attention_mask'])\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        output = (embeddings[:2] @ embeddings[2:].T)\n",
    "\n",
    "        output = output[0][0]\n",
    "        output = output.detach().numpy()\n",
    "        scores.append(output)\n",
    "    #mean_outputs.append(sum(scores) / len(scores))\n",
    "    #max_outputs.append(max(scores))\n",
    "    min_outputs.append(min(scores))\n",
    "    min_captions.append(scores.index(min(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:a woman wearing a clown mask and holding a cell phone:0.7296301126480103\n",
      "2:a horse statue on the side of a road:0.7225256562232971\n",
      "3:a red and white water fountain with a red and white clock on it:0.7212255597114563\n",
      "4:a woman with red hair and red eyes:0.7754348516464233\n",
      "5:a herd of sheep standing on top of a dirt field:0.7721688747406006\n",
      "6:three swans are swimming in the water near a body of water:0.8653014898300171\n",
      "7:a statue of a man and a horse on a beach:0.7565993666648865\n",
      "8:a beautiful young woman in a pink dress posing for a picture:0.7355852127075195\n",
      "9:a table topped with plates of cake and flowers:0.7873200178146362\n",
      "10:a train on a train track near a bridge:0.8416134119033813\n",
      "11:a man in a black and white photo:0.7619758248329163\n",
      "12:a woman sitting on a bench next to a painting:0.7826555967330933\n",
      "13:a woman standing on a sidewalk holding a tennis racquet:0.8046223521232605\n",
      "14:a woman sitting on the grass with an umbrella:0.7642844915390015\n",
      "15:a blue and white bird sitting on top of a tree branch:0.7494968771934509\n",
      "16:a white truck parked in a grassy field next to a white car:0.8326575756072998\n",
      "17:a large building with a clock tower on top of it:0.7818746566772461\n",
      "18:three different types of vegetables on a cutting board:0.782238245010376\n",
      "19:a large white boat in the middle of the ocean:0.780438244342804\n",
      "20:a large building with a large clock on it:0.7655860185623169\n",
      "21:a figurine of a cow with a bow on its head:0.7844208478927612\n",
      "22:two girls are standing in the street with skateboards:0.7329587936401367\n",
      "23:a man with a beard and a hat:0.7768372297286987\n",
      "24:a herd of sheep standing on top of a lush green field:0.7921801805496216\n",
      "25:a man riding on the back of a brown horse:0.7488920092582703\n",
      "26:a person walking on a beach with a surfboard:0.7388292551040649\n",
      "27:a motorcycle parked on the side of a road:0.8338789343833923\n",
      "28:a woman wearing glasses and a scarf:0.7703723907470703\n",
      "29:a bridge over a river with a train on it:0.7456738948822021\n",
      "30:a herd of sheep standing on top of a lush green field:0.7373051643371582\n",
      "31:a man in a suit and tie standing next to a cartoon character:0.7934913635253906\n",
      "comparing finished.\n",
      "1:2.3333333333333335\n",
      "2:3.0476190476190474\n",
      "3:4.095238095238095\n",
      "4:2.0476190476190474\n",
      "5:3.9523809523809526\n",
      "6:2.619047619047619\n",
      "7:3.9523809523809526\n",
      "8:4.142857142857143\n",
      "9:3.619047619047619\n",
      "10:1.5714285714285714\n",
      "11:4.095238095238095\n",
      "12:4.380952380952381\n",
      "13:1.1904761904761905\n",
      "14:2.8095238095238093\n",
      "15:3.619047619047619\n",
      "16:3.4285714285714284\n",
      "17:4.095238095238095\n",
      "18:2.6666666666666665\n",
      "19:2.238095238095238\n",
      "20:3.9047619047619047\n",
      "21:4.285714285714286\n",
      "22:4.523809523809524\n",
      "23:4.285714285714286\n",
      "24:2.6666666666666665\n",
      "25:3.761904761904762\n",
      "26:4.380952380952381\n",
      "27:3.0476190476190474\n",
      "28:3.8095238095238093\n",
      "29:3.857142857142857\n",
      "30:2.619047619047619\n",
      "31:4.523809523809524\n",
      "Final average score\n",
      "min: 0.7731636935664762, Total videos: 31\n",
      "Spearman's : -0.2654422617007622\n",
      "Kendall's : -0.17846762717933132\n",
      "Spearman's : -0.2654422617007622\n",
      "Kendall's : -0.17846762717933132\n"
     ]
    }
   ],
   "source": [
    "#print(\"mean\")\n",
    "#for i in range(len(prompt_paths)):\n",
    "#    print(f\"{i + 1}:\")\n",
    "#    for j in range(16):\n",
    "#        print(sequences[i * 16 + j])\n",
    "#    print(f\"score:{mean_outputs[i]}\")\n",
    "#print(\"max\")\n",
    "#for i in range(len(prompt_paths)):\n",
    "#    print(f\"{i + 1}:\")\n",
    "#    for j in range(16):\n",
    "#        print(sequences[i * 16 + j])\n",
    "#    print(f\"score:{max_outputs[i]}\")\n",
    "#print(\"min\")\n",
    "#for i in range(len(prompt_paths)):\n",
    "#    print(f\"{i + 1}:\")\n",
    "#    for j in range(16):\n",
    "#        print(sequences[i * 16 + j])\n",
    "#    print(f\"score:{min_outputs[i]}\")\n",
    "\n",
    "for i in range(31):\n",
    "    print(f\"{i+1}:{sequences[i*16+min_captions[i]]}:{min_outputs[i]}\")\n",
    "\n",
    "print(\"comparing finished.\")\n",
    "\n",
    "csv = pd.read_csv(\"./Ground_Truth.csv\")\n",
    "ground_score = []\n",
    "\n",
    "for i in range(len(prompt_paths)):\n",
    "    index = 'q' + str(3 * i + 2)\n",
    "    ground_score.append(csv[index].mean())\n",
    "    print(f\"{i+1}:{csv[index].mean()}\")\n",
    "\n",
    "ground_score = min_max_normalization(ground_score)\n",
    "\n",
    "print(\"Final average score\")\n",
    "#print(f\"mean: {sum(mean_outputs)/len(mean_outputs)}, Total videos: {len(mean_outputs)}\")\n",
    "#S_scores = pd.Series(mean_outputs)\n",
    "#S_ground_score = pd.Series(ground_score)\n",
    "#print(f\"Spearman's : {S_scores.corr(S_ground_score, method='spearman')}\")\n",
    "#print(f\"Kendall's : {S_scores.corr(S_ground_score, method='kendall')}\")\n",
    "#print(f\"Spearman's : {S_ground_score.corr(S_scores, method='spearman')}\")\n",
    "#print(f\"Kendall's : {S_ground_score.corr(S_scores, method='kendall')}\")\n",
    "\n",
    "#print(f\"max: {sum(max_outputs)/len(max_outputs)}, Total videos: {len(max_outputs)}\")\n",
    "#S_scores = pd.Series(max_outputs)\n",
    "#S_ground_score = pd.Series(ground_score)\n",
    "#print(f\"Spearman's : {S_scores.corr(S_ground_score, method='spearman')}\")\n",
    "#print(f\"Kendall's : {S_scores.corr(S_ground_score, method='kendall')}\")\n",
    "#print(f\"Spearman's : {S_ground_score.corr(S_scores, method='spearman')}\")\n",
    "#print(f\"Kendall's : {S_ground_score.corr(S_scores, method='kendall')}\")\n",
    "\n",
    "print(f\"min: {sum(min_outputs)/len(min_outputs)}, Total videos: {len(min_outputs)}\")\n",
    "S_scores = pd.Series(min_outputs)\n",
    "S_ground_score = pd.Series(ground_score)\n",
    "print(f\"Spearman's : {S_scores.corr(S_ground_score, method='spearman')}\")\n",
    "print(f\"Kendall's : {S_scores.corr(S_ground_score, method='kendall')}\")\n",
    "print(f\"Spearman's : {S_ground_score.corr(S_scores, method='spearman')}\")\n",
    "print(f\"Kendall's : {S_ground_score.corr(S_scores, method='kendall')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ground_score, mean_outputs)\n",
    "plt.plot(ground_score, np.poly1d(np.polyfit(ground_score, mean_outputs, 1))(ground_score))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ground_score, max_outputs)\n",
    "plt.plot(ground_score, np.poly1d(np.polyfit(ground_score, max_outputs, 1))(ground_score))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(ground_score, min_outputs)\n",
    "plt.plot(ground_score, np.poly1d(np.polyfit(ground_score, min_outputs, 1))(ground_score))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_videos = \"./アンケート/videos/\"\n",
    "video_paths = [os.path.join(dir_videos, x) for x in os.listdir(dir_videos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "test_num = len(video_paths)\n",
    "print(f\"test_num:{test_num}\")\n",
    "\n",
    "for i in tqdm(range(len(video_paths))):\n",
    "    video_path = video_paths[i]\n",
    "    sequence = predict_step([video_path])\n",
    "    sequences.append(sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(len(video_paths))):\n",
    "    prompt_path = prompt_paths[i]\n",
    "    sequence = sequences[i]\n",
    "\n",
    "    sequence = 'query: ' + str(sequence)\n",
    "    text = 'passage: ' + str(read_text_file(prompt_path))\n",
    "\n",
    "    input_texts = [sequence, sequence, text, text]\n",
    "    batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    output = model(**batch_dict)\n",
    "    embeddings = average_pool(output.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    scores = (embeddings[:2] @ embeddings[2:].T)\n",
    "    output = scores[0][0]\n",
    "    output = output.detach().numpy()\n",
    "    outputs.append(output)\n",
    "\n",
    "for i in range(len(prompt_paths)):\n",
    "    print(f\"{i + 1}:{sequences[i]}, score:{outputs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean: {sum(outputs)/len(outputs)}, Total videos: {len(outputs)}\")\n",
    "S_scores = pd.Series(outputs)\n",
    "S_ground_score = pd.Series(ground_score)\n",
    "print(f\"Spearman's : {S_scores.corr(S_ground_score, method='spearman')}\")\n",
    "print(f\"Kendall's : {S_scores.corr(S_ground_score, method='kendall')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ground_score, outputs)\n",
    "plt.plot(ground_score, np.poly1d(np.polyfit(ground_score, outputs, 1))(ground_score))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clip_score(video_path, text, model, tokenizer):\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Extract frames from the video\n",
    "    frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n",
    "        frames.append(resized_frame)\n",
    "\n",
    "    # Convert numpy arrays to tensors, change dtype to float, and resize frames\n",
    "    tensor_frames = [torch.from_numpy(frame).permute(2, 0, 1).float() for frame in frames]\n",
    "\n",
    "    # Initialize an empty tensor to store the concatenated features\n",
    "    concatenated_features = torch.tensor([], device=device)\n",
    "\n",
    "    # Generate embeddings for each frame and concatenate the features\n",
    "    with torch.no_grad():\n",
    "        for frame in tensor_frames:\n",
    "            frame_input = frame.unsqueeze(0).to(device)  # Add batch dimension and move the frame to the device\n",
    "            frame_features = model.get_image_features(frame_input)\n",
    "            concatenated_features = torch.cat((concatenated_features, frame_features), dim=0)\n",
    "\n",
    "    # Tokenize the text\n",
    "    text_tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "\n",
    "    # Convert the tokenized text to a tensor and move it to the device\n",
    "    text_input = text_tokens[\"input_ids\"].to(device)\n",
    "\n",
    "    # Generate text embeddings\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(text_input)\n",
    "\n",
    "    # Calculate the cosine similarity scores\n",
    "    concatenated_features = concatenated_features / concatenated_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    clip_score_frames = concatenated_features @ text_features.T\n",
    "\n",
    "    # Calculate the average CLIP score across all frames, reflects temporal consistency\n",
    "    clip_score_frames_avg = clip_score_frames.mean().item()\n",
    "\n",
    "    return clip_score_frames_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_videos = \"./アンケート/videos/\"\n",
    "dir_prompts =  \"./アンケート/prompts/\"\n",
    "\n",
    "video_paths = [os.path.join(dir_videos, x) for x in os.listdir(dir_videos)]\n",
    "prompt_paths = [os.path.join(dir_prompts, os.path.splitext(os.path.basename(x))[0]+'.txt') for x in video_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"./EvalCrafter/Evalcrafter/checkpoints/clip-vit-base-patch32\").to(device)\n",
    "clip_tokenizer = AutoTokenizer.from_pretrained(\"./EvalCrafter/Evalcrafter/checkpoints/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPscores = []\n",
    "\n",
    "test_num = len(video_paths)\n",
    "for i in tqdm(range(len(video_paths))):\n",
    "    count = 0\n",
    "    video_path = video_paths[i]\n",
    "    prompt_path = prompt_paths[i]\n",
    "    text = read_text_file(prompt_path)\n",
    "    score = calculate_clip_score(video_path, text, clip_model, clip_tokenizer)\n",
    "        # logging.info(f\"Vid: {os.path.basename(video_path)}, Pro: {os.path.basename(prompt_path)}, Current clip_score: {score}, Current max clip_score: {m_score}\")\n",
    "\n",
    "    CLIPscores.append(score)\n",
    "average_score = sum(CLIPscores) / len(CLIPscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(video_paths)):\n",
    "    print(f\"{i+1}, Clip Score:{CLIPscores[i]}, ground = {ground_score[i]}\")\n",
    "\n",
    "print(f\"Final average clip_score: {average_score}, Total videos: {len(CLIPscores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPSeries = pd.Series(CLIPscores)\n",
    "\n",
    "print(f\"Spearman's : {CLIPSeries.corr(S_ground_score, method='spearman')}\")\n",
    "print(f\"Kendall's : {CLIPSeries.corr(S_ground_score, method='kendall')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(CLIPSeries, S_ground_score)\n",
    "plt.plot(CLIPSeries, np.poly1d(np.polyfit(CLIPSeries, S_ground_score, 1))(CLIPSeries))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_scores = pd.Series(outputs)\n",
    "\n",
    "print(f\"Spearman's : {S_scores.corr(CLIPSeries, method='spearman')}\")\n",
    "print(f\"Kendall's : {S_scores.corr(CLIPSeries, method='kendall')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(S_scores, CLIPSeries)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
